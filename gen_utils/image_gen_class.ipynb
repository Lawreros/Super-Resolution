{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch MRI image class\n",
    "\n",
    "The purpose of this notebook is to work out a class that I an consistently use for the super-resolution and just general image training of MRI and potentially other medical imaging data types.\n",
    "\n",
    "It's become clear that everyone seems to have a different, unique, way of loading and organizing their data into a format that can be fed into a Pytorch `Dataset` for training a model. This can include creating intermediate `.png` images in order to limit memory usage/training time on personal hardware.\n",
    "\n",
    "Goals for this tool:\n",
    "1. Given an input folder, list all files that match a particular `prefix` and `suffix`\n",
    "2. Display sample images from the list for assurance\n",
    "3. Create randomly shuffled/altered images at different resolutions (gaussian blur, affine transformation, etc.)\n",
    "    - Make the aspect of saving these images optional\n",
    "4. Save any image generated in a specified format\n",
    "5. Be given an input and output folder and generate list of matching data\n",
    "6. Have locations of matching low and high resolution images (potentially a list for every x2 magnification)\n",
    "\n",
    "\n",
    "Output file labeling protocol:\n",
    "To keep things consistent I should probably create a labeling structure that is robust to future changes that I might want to make. This will most likely have to be stored in the string name, unless I want to make an intermediate file that is used as a key for the location of the files relative to the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "from matplotlib import pyplot\n",
    "from skimage.transform import rotate, AffineTransform, warp, rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The super resolution class:\n",
    "\n",
    "class sr_gen():\n",
    "    def __init__(self, inp_dir, HR_out_dir, LR_out_dir, prefix='', suffix=''):\n",
    "        self.inp_dir = inp_dir\n",
    "        self.HR_out_dir = HR_out_dir\n",
    "        self.HR_files = None\n",
    "        self.LR_out_dir = LR_out_dir\n",
    "        self.LR_files = None\n",
    "        self.inp_files = self._get_inp_(prefix, suffix)\n",
    "        self.template = self.get_template()\n",
    "\n",
    "    def _get_inp_(self, prefix='', suffix=''):\n",
    "        # Get the original files that will be used to generate everything\n",
    "        # Based on the prefix, for now this could be nifti files or png's.\n",
    "        # Will have to write a \"try\" statement in order to check for file type\n",
    "        files = []\n",
    "        for fil in os.listdir(self.inp_dir):\n",
    "            if fil.startswith(prefix) & fil.endswith(suffix):\n",
    "                files.append(fil)\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError('No applicable files found in input directory')\n",
    "\n",
    "        return files\n",
    "\n",
    "    def _get_LR_out_(self):\n",
    "        # get list of files in output directory and determine matching files\n",
    "        return os.listdir(self.LR_out_dir)\n",
    "\n",
    "    def _get_HR_out_(self):\n",
    "        return os.listdir(self.HR_out_dir)\n",
    "\n",
    "\n",
    "    def _view_sample_(self):\n",
    "        # Function which loads and displays random example image for sanity check\n",
    "        pyplot.figure()\n",
    "        f, axes = pyplot.subplots(2,2)\n",
    "        \n",
    "        indx = random.randint(0,len(self.inp_files)-1)\n",
    "        indy = random.randint(0,len(self.HR_files)-1)\n",
    "\n",
    "        axes[0,0].imshow(np.array(Image.open(self.inp_dir+self.inp_files[indx])))\n",
    "        axes[1,0].imshow(np.array(Image.open(self.HR_files[indy])))\n",
    "        axes[1,1].imshow(np.array(Image.open(self.LR_files[indy])))\n",
    "\n",
    "\n",
    "    def get_template(self):\n",
    "        # Returns dictonary of all option settings for this class\n",
    "        try:\n",
    "            return self.template\n",
    "        except:\n",
    "            return {'out_type':'png', # png, nii (?), DICOM (?)\n",
    "                    'unit':'intensity', #Whether you want RBG or Intensity/DICOM units\n",
    "                    'resolution':2,\n",
    "                    'translation_x':10,\n",
    "                    'translation_y':10,\n",
    "                    'rotation':0,\n",
    "                    'scale':2,\n",
    "                    'patch':False,\n",
    "                    'step': 10,\n",
    "                    'keep_blank':False,\n",
    "                    }\n",
    "\n",
    "    def save_template(self, temp):\n",
    "        # apply the provided template for randomization to self for access by other functions\n",
    "        self.template = temp\n",
    "\n",
    "\n",
    "    def run(self, clear=False):\n",
    "        # Run the analysis specified in the template dictionary. If clear is true then the \n",
    "        # files in the output directories will be deleted before creating the new images.\n",
    "\n",
    "        if clear:\n",
    "            print('Clearing existing output directories')\n",
    "            shutil.rmtree(self.HR_out_dir, ignore_errors=True)\n",
    "            shutil.rmtree(self.LR_out_dir, ignore_errors=True)\n",
    "\n",
    "        # Make the directories where the new files will be saved\n",
    "        os.makedirs(self.HR_out_dir, exist_ok=True)\n",
    "        os.makedirs(self.LR_out_dir, exist_ok=True)\n",
    "\n",
    "        HR_out_files = []\n",
    "        LR_out_files = []\n",
    "        \n",
    "        s = 1/self.template['resolution']\n",
    "        for im in self.inp_files:\n",
    "            im_h = np.array(Image.open(self.inp_dir + im))\n",
    "\n",
    "            # check the dimensions of the image\n",
    "            #print(f'Shape of High Resolution Image:{im_h.shape}')\n",
    "\n",
    "            im_h = self.rgb2ycrbcr(im_h)\n",
    "            im_h = im_h[:,:,0] #Just deal with intensity values at the moment because \n",
    "                                # having multiple channels throws off cv2 when saving, \n",
    "                                # since it also does BGR instead of RGB and will save a blue image\n",
    "\n",
    "            # TODO: compare cv2.resize with skimage.rescale or pytorch rescale for this\n",
    "            im_l = cv2.resize(im_h, (0,0), fx = s, fy =s, interpolation=cv2.INTER_CUBIC)\n",
    "            im_l = cv2.resize(im_l, (0,0), fx = self.template['resolution'],\n",
    "            fy=self.template['resolution'], interpolation=cv2.INTER_CUBIC)\n",
    "            # TODO: The above resizing results in values outside of the range [0, 255] due to \n",
    "            # the INTER_CUBIC method. For now I'm just clipping the values, but a more nuanced\n",
    "            # answer should be found\n",
    "            im_l = np.clip(im_l, 0, 255)\n",
    "\n",
    "            if self.template['patch']:\n",
    "                im, im_h, im_l = self.img_transform(im, im_h, im_l)\n",
    "                _ = self.img2patches(im, im_h, im_l, keep_blank = self.template['keep_blank'],save=True)\n",
    "                HR_out_files = HR_out_files + _\n",
    "\n",
    "            else:\n",
    "                _ = self.img_transform(im, im_h, im_l, save=True)\n",
    "                HR_out_files.append(_)\n",
    "        \n",
    "        #Do LR first because otherwise HR_out_files is changed\n",
    "        LR_out_files = [self.LR_out_dir + s for s in HR_out_files]\n",
    "        HR_out_files = [self.HR_out_dir + s for s in HR_out_files]\n",
    "\n",
    "        self.HR_files = HR_out_files\n",
    "        self.LR_files = LR_out_files\n",
    "\n",
    "\n",
    "    def img_transform(self,im, im_h, im_l, save=False):\n",
    "        # Transform the original files using a variety of methods\n",
    "\n",
    "        opp = '' #string for storing the operations performed on the images\n",
    "\n",
    "        # If shifting in the x or y direction was selected\n",
    "        if self.template['translation_x'] > 0 | self.template['translation_y'] > 0:\n",
    "            _a = np.random.randint(0,self.template['translation_x'])\n",
    "            _b = np.random.randint(0,self.template['translation_y'])\n",
    "            transform = AffineTransform(translation=(_a, _b))\n",
    "            im_h = warp(im_h, transform,mode='reflect')\n",
    "            im_l = warp(im_l, transform,mode='reflect')\n",
    "            opp += f'_x{_a}_y{_b}'\n",
    "\n",
    "        if self.template['scale'] > 1:\n",
    "            _a = np.random.randint(1,self.template['scale']+1)\n",
    "            transform = AffineTransform(scale=_a)\n",
    "            im_h = warp(im_h, transform, mode='reflect')\n",
    "            im_l = warp(im_l, transform,mode='reflect')\n",
    "            opp+= f'_scale{_a}'\n",
    "\n",
    "        # If rotation was selected\n",
    "        if self.template['rotation'] > 0:\n",
    "            _a = np.random.randint(0,self.template['rotation'])\n",
    "            im_h = rotate(im_h, _a, mode=\"reflect\")\n",
    "            im_l = rotate(im_l, _a, mode=\"reflect\")\n",
    "            opp+= f'_rot{_a}'\n",
    "\n",
    "        opp = im.split('.')[0] + opp\n",
    "\n",
    "        if save:\n",
    "            print(f'Saving image: {opp}')\n",
    "            cv2.imwrite(f'{self.HR_out_dir}/{opp}.png', im_h)\n",
    "            cv2.imwrite(f'{self.LR_out_dir}/{opp}.png', im_l)\n",
    "            return opp\n",
    "        else:\n",
    "            return opp, im_h, im_l\n",
    "\n",
    "    def img2patches(self, im, im_h, im_l=False, keep_blank=False, save=False):\n",
    "        # Take a given image and generate patches and returns a stack of images\n",
    "        # im : str, name of the image being cut into patches\n",
    "        # im_h : ndarray, numpy array of the high-resolution image\n",
    "        # im_l : ndarray, numpy array of the low-resolution image. If this is false, \n",
    "        # then you only want to make patches from one image (in this case im_h)\n",
    "\n",
    "        patch_size = self.template['patch']\n",
    "        step = self.template['step']\n",
    "\n",
    "\n",
    "        # Get the height and width of the provided images\n",
    "        h_h, w_h = im_h.shape\n",
    "        h_l, w_l = im_l.shape\n",
    "\n",
    "        # Create a numpy stack following Pytorch protocols\n",
    "        HR_stack = np.zeros((len(range(0,w_h,step))*len(range(0,h_h,step)),patch_size,patch_size))\n",
    "        if isinstance(im_l, np.ndarray):\n",
    "            LR_stack = np.zeros((len(range(0,w_h,step))*len(range(0,h_h,step)),patch_size,patch_size))\n",
    "\n",
    "        im_name = im.split('.')[0]\n",
    "\n",
    "        cnt = 0\n",
    "        blank = 0\n",
    "\n",
    "        for i in range(0,w_h,step):\n",
    "            for j in range(0,h_h,step):\n",
    "                if i+patch_size < w_h and j+patch_size < h_h:\n",
    "\n",
    "                    sample_h = im_h[j:j+patch_size, i: i+patch_size]\n",
    "                    if isinstance(im_l, np.ndarray):\n",
    "                        sample_l = im_l[j:j+patch_size, i:i+patch_size]\n",
    "\n",
    "                    # if you've chosen to keep blank patches or if the patch is not blank add \n",
    "                    # it to the stack\n",
    "                    if keep_blank or (sample_h.max() > 0 or sample_l.max() > 0):\n",
    "                        HR_stack[cnt, :,:] = sample_h\n",
    "                        if isinstance(im_l, np.ndarray):\n",
    "                            LR_stack[cnt,:,:] = sample_l\n",
    "                        cnt += 1\n",
    "                    else:\n",
    "                        blank += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Return a list of image names and numpy array with the first cnt layers\n",
    "        HR_fnames = []\n",
    "        for i in range(cnt):\n",
    "            HR_fnames.append(f'{im_name}_{i}.png')\n",
    "\n",
    "        if save: #Whether to save a patch if it is blank/intensity value of 0\n",
    "            for i in range(cnt):\n",
    "                cv2.imwrite(f'{self.HR_out_dir}/{im_name}_{i}.png', HR_stack[i,:,:])\n",
    "                if isinstance(im_l, np.ndarray):\n",
    "                    cv2.imwrite(f'{self.LR_out_dir}/{im_name}_{i}.png', LR_stack[i,:,:])\n",
    "            return HR_fnames\n",
    "        \n",
    "        else:\n",
    "            if isinstance(im_l, np.ndarray):\n",
    "                return HR_fnames, HR_stack[:cnt,:,:], LR_stack[:cnt,:,:]\n",
    "            else:\n",
    "                return HR_fnames, HR_stack[:cnt,:,:]\n",
    "        \n",
    "\n",
    "\n",
    "    def rgb2ycrbcr(self, img_rgb):\n",
    "        # Takes an RBG image and returns it as a YCRBCR image (if you just want to focus\n",
    "        #  on luminance values of an image)\n",
    "\n",
    "        img_rgb = img_rgb.astype(np.float32)\n",
    "\n",
    "        img_ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2YCR_CB)\n",
    "        img_ycbcr = img_ycrcb[:,:,(0,2,1)].astype(np.float32)\n",
    "        img_ycbcr[:,:,0] = (img_ycbcr[:,:,0]*(235-16)+16)/255.0\n",
    "        img_ycbcr[:,:,1:] = (img_ycbcr[:,:,1:]*(240-16)+16)/255.0\n",
    "\n",
    "        return img_ycbcr\n",
    "\n",
    "\n",
    "    def load_image_pair(self, im_id):\n",
    "        # A method which loads the provided file and returns a numpy array\n",
    "        # this is used because it will remember in the template dictionary how\n",
    "        # the images were saved (either as RBG or intensity values or 3D array). \n",
    "        # This will help minimize headaches caused by different image types.\n",
    "\n",
    "        # im_id can either be the index value or the name of the file\n",
    "        if isinstance(im_id, int):\n",
    "            HR_file = self.HR_files[im_id]\n",
    "            LR_file = self.LR_files[im_id]\n",
    "        elif isinstance(im_id, str):\n",
    "             _ = self.HR_files.index(im_id)\n",
    "             HR_file = self.HR_files[_]\n",
    "             LR_file = self.LR_files[_]\n",
    "        else:\n",
    "            TypeError(\"Invalid image identifier, please input a string to integer\")\n",
    "\n",
    "        # Check what data type to load from the template\n",
    "        if self.template['out_type'] == 'png':\n",
    "            im_h = np.array(Image.open(HR_file))\n",
    "            im_l = np.array(Image.open(LR_file))\n",
    "\n",
    "\n",
    "        return im_h, im_l\n",
    "\n",
    "    def match_altered(self, update=True, paths=False):\n",
    "        # Get the files that have been generated in the output directory\n",
    "        # If update is false, then just return a list of matched names, if true then\n",
    "        # change the class variable values accordingly.\n",
    "        hr_files = self._get_HR_out_()\n",
    "        lr_files = self._get_LR_out_()\n",
    "\n",
    "        # Get a set of all the files with agreement before the metadata\n",
    "        if len(hr_files) > len(lr_files):\n",
    "            matches = list(set(hr_files)-(set(hr_files)-set(lr_files)))\n",
    "        else:\n",
    "            matches = list(set(lr_files)-(set(lr_files)-set(hr_files)))\n",
    "\n",
    "        if update:\n",
    "            # If you want to save these matched files as class variables\n",
    "            self.HR_files = [self.HR_out_dir + _ for _ in matches]\n",
    "            self.LR_files = [self.LR_out_dir + _ for _ in matches]\n",
    "            print('HR and LR file locations updated')\n",
    "        \n",
    "        if paths:\n",
    "            return self.HR_files, self.LR_files\n",
    "\n",
    "    def change_out(self, HR_out_dir, LR_out_dir):\n",
    "        # Change the output locations so you can save into a new file\n",
    "        self.HR_out_dir = HR_out_dir\n",
    "        self.HR_files = None\n",
    "        self.LR_out_dir = LR_out_dir\n",
    "        self.LR_files = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = sr_gen('./data/raw/nii_sub_HR/','./data/raw/HR_output/','./data/raw/LR_output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qq.match_altered(True)\n",
    "#pyplot.imshow(qq.load_image_pair(4)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = qq.get_template()\n",
    "temp[\"patch\"]=20\n",
    "temp[\"step\"]=60\n",
    "qq.save_template(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq.run(clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq.match_altered(update=True)\n",
    "qq._view_sample_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayim = np.zeros(im_h.shape)\n",
    "grayim[:,:,0]= grayim[:,:,1] = grayim[:,:,2] = im_h[:,:,0]\n",
    "\n",
    "cv2.imwrite('test.png',grayim[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(Image.open('./test.png')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Dataloader and NN model\n",
    "Here we'll make sure the above class works in a way that is compatible with Pytorch's Dataloader and ML modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: There's not reasing I can't combine the Dataset class and my custom class into one thing\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sr_class):\n",
    "        self.sr_class = sr_class\n",
    "\n",
    "        # In case I forget to run match_altered before pulling the class\n",
    "        if not sr_class.HR_files:\n",
    "            sr_class.match_altered(update=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sr_class.HR_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        Y, X = self.sr_class.load_image_pair(index)\n",
    "        X = torch.unsqueeze(torch.tensor(X, dtype=torch.float32),0)\n",
    "        Y = torch.unsqueeze(torch.tensor(Y, dtype=torch.float32),0)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "training_set = Dataset(qq)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCN(nn.Module):\n",
    "    def __init__(self,sy,sg, model_file=False, train=True):\n",
    "        super().__init__()\n",
    "        C = 5\n",
    "        L = 5\n",
    "\n",
    "        Dx = torch.normal(0,1, size = (25,128))\n",
    "        Dy = torch.normal(0,1, size = (100,128))\n",
    "        I = torch.eye(128)\n",
    "\n",
    "        self.conv = nn.Conv2d(1,100,9, bias = False, stride =1, padding = 6)\n",
    "        self.mean2 = nn.Conv2d(1,1,13, bias = False, stride = 1, padding = 6)\n",
    "        self.diffms = nn.Conv2d(1,25,9, bias=False, stride = 1, padding=6)\n",
    "\n",
    "        self.wd = nn.Conv2d(100,128,1,bias = False, stride = 1)\n",
    "        self.usd1 = nn.Conv2d(128, 128, 1, bias = False, stride=1)\n",
    "        self.ud = nn.Conv2d(128,25,1,bias=False,stride=1)\n",
    "        self.addp = nn.Conv2d(16,1,1, bias = False, stride = 1)\n",
    "\n",
    "        if train: #If you are currently training the model\n",
    "            self.mean2.weight = torch.nn.Parameter(self.create_gaus(13), requires_grad = False)\n",
    "            self.diffms.weight = torch.nn.Parameter(self.create_diffms(9,5),requires_grad=False)\n",
    "            self.wd.weight = torch.nn.Parameter(self.expand_params(C*Dy.T), requires_grad=True)\n",
    "            self.usd1.weight = torch.nn.Parameter(self.expand_params(I - torch.matmul(Dy.T,Dy)), requires_grad=True)\n",
    "            self.ud.weight = torch.nn.Parameter(self.expand_params((1/(C*L))*Dx), requires_grad=True)\n",
    "            self.addp.weight = torch.nn.Parameter(torch.ones(1,16,1,1)*0.06, requires_grad=True)\n",
    "\n",
    "        else:\n",
    "            self.conv.weight = torch.nn.Parameter(torch.ones(100,1,9,9),requires_grad=False)\n",
    "            self.mean2.weight = torch.nn.Parameter(self.create_gaus(13),requires_grad=False)\n",
    "            self.diffms.weight = torch.nn.Parameter(self.create_diffms(9,5),requires_grad=False)\n",
    "            self.wd.weight = torch.nn.Parameter(self.expand_params(C*Dy.T),requires_grad=False)\n",
    "            self.usd1.weight = torch.nn.Parameter(self.expand_params(I - torch.matmul(Dy.T,Dy)),requires_grad=False)\n",
    "            self.ud.weight = torch.nn.Parameter(self.expand_params((1/(C*L))*Dx),requires_grad=False)\n",
    "            self.addp.weight = torch.nn.Parameter(torch.ones(1,16,1,1)*0.06,requires_grad=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, k, sy=9, sg=5):\n",
    "        im_mean = self.mean2(x)\n",
    "        # print(f'im_mean shape {im_mean.shape}')\n",
    "        diffms = self.diffms(x)\n",
    "        # print(f'diffms shape: {diffms.shape}')\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "        # y = torch.zeros(n, 100, h-8, w-8)\n",
    "        x = self.conv(x)\n",
    "        # print(f'post conv shape {x.shape}')\n",
    "        #print(f'conv max {x.max()}')\n",
    "        x=x+1\n",
    "\n",
    "        x = x/torch.linalg.vector_norm(x, ord=2, dim=1, keepdim=True)\n",
    "        # print(f'post vector norm shape: {x.shape}')\n",
    "        #print(f'postnorm max {x.max()}')\n",
    "\n",
    "        x = self.wd(x)\n",
    "        #print(f'conv wd {x.max()}')\n",
    "        z = self.ShLU(x,1)\n",
    "        #print(f'conv SHLU {x.max()}')\n",
    "\n",
    "        # Go through LISTA\n",
    "        for i in range(k):\n",
    "            z = self.ShLU(self.usd1(z)+x,1)\n",
    "\n",
    "        x = self.ud(z)\n",
    "        #print(f'ud max {x.max()}')\n",
    "        # print(f'post ud shape {x.shape}')\n",
    "        x = (x/torch.linalg.vector_norm(x, ord=2, dim=1, keepdim=True))*torch.linalg.vector_norm(diffms, ord=2, dim=1, keepdim=True)*1.1\n",
    "        # print(f'prereassembled x shape {x.shape}')\n",
    "        x = self.reassemble2(x,im_mean,4)\n",
    "        # print(f'reassembled x shape {x.shape}')\n",
    "        x = self.addp(x)\n",
    "        #print(f'x.reassemble.max = {x.max()}')\n",
    "        x = x+im_mean\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reassemble2(self, x, im_mean, patch_size):\n",
    "        img = im_mean\n",
    "        s, c, h, w = img.shape\n",
    "        \n",
    "        # img_stack=torch.zeros(s,25,h,w)\n",
    "        img_stack=torch.zeros(s,16,h,w)\n",
    "        \n",
    "        #go through every sample and reassemble the image\n",
    "        for q in range(x.shape[0]):\n",
    "            filt = 0\n",
    "            for ii in range(patch_size-1, -1, -1):\n",
    "                for jj in range(patch_size-1, -1, -1):\n",
    "                    img_stack[q,filt,:,:] = x[q,filt,jj:(jj+h), ii:(ii+w)]\n",
    "                    filt+=1\n",
    "        \n",
    "        return img_stack\n",
    "    \n",
    "    def create_diffms(self, kern_size, sy=5):\n",
    "        diffms = torch.zeros(sy**2,1,kern_size,kern_size)\n",
    "        \n",
    "        neg = -1*(1/(sy**2))\n",
    "        pos = 1+neg\n",
    "        \n",
    "        border = int((kern_size-sy)/2)\n",
    "        base = torch.zeros(sy,sy)+neg\n",
    "        cnt=0\n",
    "        \n",
    "        for i in range(sy**2):\n",
    "            base = torch.zeros(sy**2)+neg\n",
    "            base[cnt]=pos\n",
    "            diffms[i,0,border:(kern_size-border),border:(kern_size-border)] = base.reshape([sy,sy])\n",
    "            cnt+=1\n",
    "        return diffms\n",
    "    \n",
    "    \n",
    "    def create_gaus(self, kern_size, sy=9,std=2.15):\n",
    "        n = torch.arange(0,sy)-(sy-1.0)/2.0\n",
    "        sig2 = 2 * std * std\n",
    "        gkern1d = torch.exp(-n ** 2 / sig2)\n",
    "        gkern1d = gkern1d/torch.sum(gkern1d)\n",
    "        #print(gkern1d.shape)\n",
    "        gkern2d = torch.outer(gkern1d, gkern1d)\n",
    "    \n",
    "\n",
    "        # Wrap in zeros, if kern_size > sy\n",
    "        gaussian_filter = torch.zeros(1,1,kern_size,kern_size)\n",
    "        border = int((kern_size-sy)/2)\n",
    "        gaussian_filter[0,0,border:(kern_size-border),border:(kern_size-border)] = gkern2d#(sy,std=std)\n",
    "        #print(gaussian_filter.shape)\n",
    "        return gaussian_filter\n",
    "        \n",
    "    \n",
    "    def fixed_positions(self, tens, mult, sg):\n",
    "        f, _ , h, w = tens.shape\n",
    "        new_filt = torch.zeros(f*mult, 1, sg,sg)\n",
    "        cnt = 0\n",
    "        filt = 0\n",
    "        \n",
    "        for filt in range(f):\n",
    "            for j in range((sg-w)+1):\n",
    "                for i in range((sg-h)+1):\n",
    "                    new_filt[cnt,0,i:i+h,j:j+w] = tens[filt]\n",
    "                    cnt+=1\n",
    "        return new_filt\n",
    "    \n",
    "    def expand_params(self,tens):\n",
    "        return torch.unsqueeze(torch.unsqueeze(tens,2),3)\n",
    "    \n",
    "    def ShLU(self,a, th):\n",
    "        return torch.sign(a)*torch.maximum(abs(a)-th, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SCN(9,5,train=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        {\"params\": net.addp.parameters()},#, \"lr\": 0.0002, \"momentum\": 0.00005},\n",
    "        {\"params\": net.conv.parameters()},#, \"lr\": 0.0003, \"momentum\": 0.0001},\n",
    "        {\"params\": net.wd.parameters()},\n",
    "        {\"params\": net.usd1.parameters()},\n",
    "        {\"params\": net.ud.parameters()},\n",
    "    ],\n",
    "    lr=0.00007, momentum = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "# Loop over epochs\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    losses = []\n",
    "    losses_per = []\n",
    "\n",
    "    # Training\n",
    "    count = 0\n",
    "    for inp, goal in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(inp,2) # the 2 is the number of iterations in the LISTA network\n",
    "        print(output.shape)\n",
    "        output = torch.clamp(output, 0, 255)\n",
    "\n",
    "        loss = criterion(output,goal)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'loss = {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "        print(f'mini-batch # {count}, mean loss = {sum(losses)/len(losses)}')\n",
    "        count = count+1\n",
    "\n",
    "    torch.save(net.state_dict(), f'./MRI_save_{epoch}.p')\n",
    "    print(f'\\n\\n epoch {epoch}, loss mean: {sum(losses)/len(losses)}, loss: {min(losses)}-{max(losses)}\\n')\n",
    "\n",
    "    # Give computer time to cool down\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539b544e2c3fdc58492248d082a132f5e0b4fea63e914fb274c32873997cf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
