{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-Resolution Image Generation Class\n",
    "\n",
    "The purpose of this notebook is to develop a class which can be consistently use for image handling when working with Super-Resolution (SR) models (mainly on MRI or other 3D medical images). Since the problem that Super-Resolution models are aiming to solve is fairly consistent, there should be significant overlap in how testing and training images are created/handled.\n",
    "\n",
    "\n",
    "### DISCLAIMER:\n",
    "This class is meant to make playing around with SR models easier on *personal* computing hardware. This class is not designed with large computing resources in mind and will likely be less efficient than other methods. There are three particular issues related to playing around with SR that this class is meant to address:\n",
    "1. **Overhead involved with augmentation of training and testing images**. When bug-testing and playing around with hyperparameters, the overhead of augmentating images, taking patches, etc. add time that isn't necessary. By generating these images/sub-images ahead of time the local memeory is leveraged to skip these steps each time you re-run after changing something. Once everything is working, it's easy to keep generating new augmented images as the model is trained.\n",
    "\n",
    "2. **RAM limitations on personal devices**. Medical/3D images can be painfuly large. If you are training a model on sub-samples/patches of larger files, you either have to only sample from a subset of images for a batch, then load another subset for the next batch (hindering how randomly sampled the contents of each batch are), or inefficiently load images randomly only to take 1-2 sub-samples/patches from them. By saving these sub-samples/patches as seperate files ahead of time, random sampling is easier and less demanding of resources.\n",
    "\n",
    "3. **Data organization**. It is very easy to get turned around when generating the low-resolution and high-resolution image pairs for training and testing. This class enforces a consistent organizational method, which allows it to take inventory and find matching images from previously generated images.\n",
    "\n",
    "\n",
    "### Functionality:\n",
    "To prevent this class from becoming a bloated mess of functionality and niche features, the ideal capabilities of this class will be cataloged here. The goals for this tool are:\n",
    "1. Given an input directory, list all files that match a particular `prefix` and `suffix`\n",
    "    - Detect if the directory(s) given were generated by this class previously\n",
    "2. Load common image types:\n",
    "    - [X] `.png`\n",
    "    - [X] `.nii` and `.nii.gz`\n",
    "    - [] `.dcm`\n",
    "3. Support several different methods of downsampling and upsampling:\n",
    "    - [X] bilinear interpolation\n",
    "    - [] trilinear interpolation\n",
    "    - [] \"nearest\" (see pytorch torchvision)\n",
    "4. Create randomly shuffled/altered images at different resolutions (gaussian blur, affine transformation, etc.)\n",
    "    - Make the aspect of saving these images optional so it can be used as a `generator` if possible\n",
    "5. Display sample images from the generated images for assurance when prompted\n",
    "6. Save files in multiple formats, when relevant:\n",
    "    - [X] 2D full image (potentially transformed)\n",
    "    - [X] 2D patch\n",
    "    - [X] 3D full image (potentially transformed)\n",
    "    - [X] 3D \"patch\", or a sub-volume for the \n",
    "7. Save any image generated in a specified format\n",
    "8. Have locations of matching low and high resolution images\n",
    "9. Generate file containing the values used in the template for future revisting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "import math as math\n",
    "import nibabel as nib\n",
    "import pydicom\n",
    "from matplotlib import pyplot\n",
    "from skimage.transform import rotate, AffineTransform, warp, rescale, resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The super resolution class:\n",
    "\n",
    "class sr_gen():\n",
    "    def __init__(self, inp_dir, HR_out_dir, LR_out_dir, prefix='', suffix=''):\n",
    "        self.inp_dir = inp_dir\n",
    "        self.HR_out_dir = HR_out_dir\n",
    "        self.HR_files = None\n",
    "        self.LR_out_dir = LR_out_dir\n",
    "        self.LR_files = None\n",
    "        self.inp_files = self._get_inp_(prefix, suffix)\n",
    "        self.template = self.get_template()\n",
    "\n",
    "    def _get_inp_(self, prefix='', suffix=''):\n",
    "        # Get the original files that will be used to generate everything\n",
    "        # Based on the prefix, for now this could be nifti files or png's.\n",
    "        # Will have to write a \"try\" statement in order to check for file type\n",
    "        files = []\n",
    "        for fil in os.listdir(self.inp_dir):\n",
    "            if fil.startswith(prefix) & fil.endswith(suffix):\n",
    "                files.append(fil)\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError('No applicable files found in input directory')\n",
    "\n",
    "        return files\n",
    "\n",
    "    def _get_LR_out_(self):\n",
    "        # get list of files in output directory and determine matching files\n",
    "        return os.listdir(self.LR_out_dir)\n",
    "\n",
    "    def _get_HR_out_(self):\n",
    "        return os.listdir(self.HR_out_dir)\n",
    "\n",
    "\n",
    "    def _view_sample_(self):\n",
    "        # Function which loads and displays random example image for sanity check\n",
    "        pyplot.figure()\n",
    "        f, axes = pyplot.subplots(2,2)\n",
    "        \n",
    "        indx = random.randint(0,len(self.inp_files)-1)\n",
    "        indy = random.randint(0,len(self.HR_files)-1)\n",
    "\n",
    "        axes[0,0].imshow(np.array(Image.open(self.inp_dir+self.inp_files[indx])))\n",
    "        axes[1,0].imshow(np.array(Image.open(self.HR_files[indy])))\n",
    "        axes[1,1].imshow(np.array(Image.open(self.LR_files[indy])))\n",
    "\n",
    "\n",
    "    def get_template(self):\n",
    "        # Returns dictonary of all option settings for this class\n",
    "        try:\n",
    "            return self.template\n",
    "        except:\n",
    "            return {'out_type':'png', # png, nii (?), DICOM (?)\n",
    "                    'unit':'intensity', #Whether you want RBG or Intensity/DICOM units\n",
    "                    'resolution':2,\n",
    "                    'translation_x':10,\n",
    "                    'translation_y':10,\n",
    "                    'rotation':0,\n",
    "                    'scale':2,\n",
    "                    'patch':False,\n",
    "                    'step': 10,\n",
    "                    'keep_blank':False,\n",
    "                    }\n",
    "\n",
    "    def save_template(self, temp):\n",
    "        # apply the provided template for randomization to self for access by other functions\n",
    "        self.template = temp\n",
    "\n",
    "\n",
    "    def run(self, clear=False):\n",
    "        # Run the analysis specified in the template dictionary. If clear is true then the \n",
    "        # files in the output directories will be deleted before creating the new images.\n",
    "\n",
    "        if clear:\n",
    "            print('Clearing existing output directories')\n",
    "            shutil.rmtree(self.HR_out_dir, ignore_errors=True)\n",
    "            shutil.rmtree(self.LR_out_dir, ignore_errors=True)\n",
    "\n",
    "        # Make the directories where the new files will be saved\n",
    "        os.makedirs(self.HR_out_dir, exist_ok=True)\n",
    "        os.makedirs(self.LR_out_dir, exist_ok=True)\n",
    "\n",
    "        HR_out_files = []\n",
    "        LR_out_files = []\n",
    "        \n",
    "        s = 1/self.template['resolution']\n",
    "        for im in self.inp_files:\n",
    "            im_h = np.array(Image.open(self.inp_dir + im))\n",
    "\n",
    "            # check the dimensions of the image\n",
    "            #print(f'Shape of High Resolution Image:{im_h.shape}')\n",
    "\n",
    "            im_h = self.rgb2ycrbcr(im_h)\n",
    "            im_h = im_h[:,:,0] #Just deal with intensity values at the moment because \n",
    "                                # having multiple channels throws off cv2 when saving, \n",
    "                                # since it also does BGR instead of RGB and will save a blue image\n",
    "\n",
    "            # TODO: compare cv2.resize with skimage.rescale or pytorch rescale for this\n",
    "            im_l = cv2.resize(im_h, (0,0), fx = s, fy =s, interpolation=cv2.INTER_CUBIC)\n",
    "            im_l = cv2.resize(im_l, (0,0), fx = self.template['resolution'],\n",
    "            fy=self.template['resolution'], interpolation=cv2.INTER_CUBIC)\n",
    "            # TODO: The above resizing results in values outside of the range [0, 255] due to \n",
    "            # the INTER_CUBIC method. For now I'm just clipping the values, but a more nuanced\n",
    "            # answer should be found\n",
    "            im_l = np.clip(im_l, 0, 255)\n",
    "\n",
    "            if self.template['patch']:\n",
    "                im, im_h, im_l = self.img_transform(im, im_h, im_l)\n",
    "                _ = self.img2patches(im, im_h, im_l, keep_blank = self.template['keep_blank'],save=True)\n",
    "                HR_out_files = HR_out_files + _\n",
    "\n",
    "            else:\n",
    "                _ = self.img_transform(im, im_h, im_l, save=True)\n",
    "                HR_out_files.append(_)\n",
    "        \n",
    "        #Do LR first because otherwise HR_out_files is changed\n",
    "        LR_out_files = [self.LR_out_dir + s for s in HR_out_files]\n",
    "        HR_out_files = [self.HR_out_dir + s for s in HR_out_files]\n",
    "\n",
    "        self.HR_files = HR_out_files\n",
    "        self.LR_files = LR_out_files\n",
    "\n",
    "\n",
    "    def img_transform(self,im, im_h, im_l, save=False):\n",
    "        # Transform the original files using a variety of methods\n",
    "\n",
    "        opp = '' #string for storing the operations performed on the images\n",
    "\n",
    "        # If shifting in the x or y direction was selected\n",
    "        if self.template['translation_x'] > 0 | self.template['translation_y'] > 0:\n",
    "            _a = np.random.randint(0,self.template['translation_x'])\n",
    "            _b = np.random.randint(0,self.template['translation_y'])\n",
    "            transform = AffineTransform(translation=(_a, _b))\n",
    "            im_h = warp(im_h, transform,mode='reflect')\n",
    "            im_l = warp(im_l, transform,mode='reflect')\n",
    "            opp += f'_x{_a}_y{_b}'\n",
    "\n",
    "        if self.template['scale'] > 1:\n",
    "            _a = np.random.randint(1,self.template['scale']+1)\n",
    "            #transform = AffineTransform(scale=_a)\n",
    "            im_h = rescale(im_h, scale = _a, mode='reflect')\n",
    "            im_l = rescale(im_l, scale = _a, mode='reflect')\n",
    "            opp+= f'_scale{_a}'\n",
    "\n",
    "        # If rotation was selected\n",
    "        if self.template['rotation'] > 0:\n",
    "            _a = np.random.randint(-self.template['rotation'],self.template['rotation'])\n",
    "            im_h = rotate(im_h, _a, mode=\"reflect\")\n",
    "            im_l = rotate(im_l, _a, mode=\"reflect\")\n",
    "            opp+= f'_rot{_a}'\n",
    "\n",
    "        opp = im.split('.')[0] + opp\n",
    "\n",
    "        if save:\n",
    "            print(f'Saving image: {opp}')\n",
    "            cv2.imwrite(f'{self.HR_out_dir}/{opp}.png', im_h)\n",
    "            cv2.imwrite(f'{self.LR_out_dir}/{opp}.png', im_l)\n",
    "            return opp\n",
    "        else:\n",
    "            return opp, im_h, im_l\n",
    "\n",
    "    def img2patches(self, im, im_h, im_l=False, keep_blank=False, save=False):\n",
    "        # Take a given image and generate patches and returns a stack of images\n",
    "        # im : str, name of the image being cut into patches\n",
    "        # im_h : ndarray, numpy array of the high-resolution image\n",
    "        # im_l : ndarray, numpy array of the low-resolution image. If this is false, \n",
    "        # then you only want to make patches from one image (in this case im_h)\n",
    "\n",
    "        patch_size = self.template['patch']\n",
    "        step = self.template['step']\n",
    "\n",
    "\n",
    "        # Get the height and width of the provided images\n",
    "        h_h, w_h = im_h.shape\n",
    "        h_l, w_l = im_l.shape\n",
    "\n",
    "        # Create a numpy stack following Pytorch protocols\n",
    "        HR_stack = np.zeros((len(range(0,w_h,step))*len(range(0,h_h,step)),patch_size,patch_size))\n",
    "        if isinstance(im_l, np.ndarray):\n",
    "            LR_stack = np.zeros((len(range(0,w_h,step))*len(range(0,h_h,step)),patch_size,patch_size))\n",
    "\n",
    "        im_name = im.split('.')[0]\n",
    "\n",
    "        cnt = 0\n",
    "        blank = 0\n",
    "\n",
    "        for i in range(0,w_h,step):\n",
    "            for j in range(0,h_h,step):\n",
    "                if i+patch_size < w_h and j+patch_size < h_h:\n",
    "\n",
    "                    sample_h = im_h[j:j+patch_size, i: i+patch_size]\n",
    "                    if isinstance(im_l, np.ndarray):\n",
    "                        sample_l = im_l[j:j+patch_size, i:i+patch_size]\n",
    "\n",
    "                    # if you've chosen to keep blank patches or if the patch is not blank add \n",
    "                    # it to the stack\n",
    "                    if keep_blank or (sample_h.max() > 0 or sample_l.max() > 0):\n",
    "                        HR_stack[cnt, :,:] = sample_h\n",
    "                        if isinstance(im_l, np.ndarray):\n",
    "                            LR_stack[cnt,:,:] = sample_l\n",
    "                        cnt += 1\n",
    "                    else:\n",
    "                        blank += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Return a list of image names and numpy array with the first cnt layers\n",
    "        HR_fnames = []\n",
    "        for i in range(cnt):\n",
    "            HR_fnames.append(f'{im_name}_{i}.png')\n",
    "\n",
    "        if save: #Whether to save a patch if it is blank/intensity value of 0\n",
    "            for i in range(cnt):\n",
    "                cv2.imwrite(f'{self.HR_out_dir}/{im_name}_{i}.png', HR_stack[i,:,:])\n",
    "                if isinstance(im_l, np.ndarray):\n",
    "                    cv2.imwrite(f'{self.LR_out_dir}/{im_name}_{i}.png', LR_stack[i,:,:])\n",
    "            return HR_fnames\n",
    "        \n",
    "        else:\n",
    "            if isinstance(im_l, np.ndarray):\n",
    "                return HR_fnames, HR_stack[:cnt,:,:], LR_stack[:cnt,:,:]\n",
    "            else:\n",
    "                return HR_fnames, HR_stack[:cnt,:,:]\n",
    "        \n",
    "\n",
    "\n",
    "    def rgb2ycrbcr(self, img_rgb):\n",
    "        # Takes an RBG image and returns it as a YCRBCR image (if you just want to focus\n",
    "        #  on luminance values of an image)\n",
    "\n",
    "        img_rgb = img_rgb.astype(np.float32)\n",
    "\n",
    "        img_ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2YCR_CB)\n",
    "        img_ycbcr = img_ycrcb[:,:,(0,2,1)].astype(np.float32)\n",
    "        img_ycbcr[:,:,0] = (img_ycbcr[:,:,0]*(235-16)+16)/255.0\n",
    "        img_ycbcr[:,:,1:] = (img_ycbcr[:,:,1:]*(240-16)+16)/255.0\n",
    "\n",
    "        return img_ycbcr\n",
    "\n",
    "\n",
    "    def load_image_pair(self, im_id):\n",
    "        # A method which loads the provided file and returns a numpy array\n",
    "        # this is used because it will remember in the template dictionary how\n",
    "        # the images were saved (either as RBG or intensity values or 3D array). \n",
    "        # This will help minimize headaches caused by different image types.\n",
    "\n",
    "        # im_id can either be the index value or the name of the file\n",
    "        if isinstance(im_id, int):\n",
    "            HR_file = self.HR_files[im_id]\n",
    "            LR_file = self.LR_files[im_id]\n",
    "        elif isinstance(im_id, str):\n",
    "             _ = self.HR_files.index(im_id)\n",
    "             HR_file = self.HR_files[_]\n",
    "             LR_file = self.LR_files[_]\n",
    "        else:\n",
    "            TypeError(\"Invalid image identifier, please input a string to integer\")\n",
    "\n",
    "        # Check what data type to load from the template\n",
    "        if self.template['out_type'] == 'png':\n",
    "            im_h = np.array(Image.open(HR_file))\n",
    "            im_l = np.array(Image.open(LR_file))\n",
    "\n",
    "\n",
    "        return im_h, im_l\n",
    "\n",
    "    def match_altered(self, update=True, paths=False):\n",
    "        # Get the files that have been generated in the output directory\n",
    "        # If update is false, then just return a list of matched names, if true then\n",
    "        # change the class variable values accordingly.\n",
    "        hr_files = self._get_HR_out_()\n",
    "        lr_files = self._get_LR_out_()\n",
    "\n",
    "        # Get a set of all the files with agreement before the metadata\n",
    "        if len(hr_files) > len(lr_files):\n",
    "            matches = list(set(hr_files)-(set(hr_files)-set(lr_files)))\n",
    "        else:\n",
    "            matches = list(set(lr_files)-(set(lr_files)-set(hr_files)))\n",
    "\n",
    "        if update:\n",
    "            # If you want to save these matched files as class variables\n",
    "            self.HR_files = [self.HR_out_dir + _ for _ in matches]\n",
    "            self.LR_files = [self.LR_out_dir + _ for _ in matches]\n",
    "            print('HR and LR file locations updated')\n",
    "        \n",
    "        if paths:\n",
    "            return self.HR_files, self.LR_files\n",
    "\n",
    "    def change_out(self, HR_out_dir, LR_out_dir):\n",
    "        # Change the output locations so you can save into a new file\n",
    "        self.HR_out_dir = HR_out_dir\n",
    "        self.HR_files = None\n",
    "        self.LR_out_dir = LR_out_dir\n",
    "        self.LR_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = sr_gen('./data/raw/nii_sub_HR/','./data/raw/HR_output/','./data/raw/LR_output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qq.match_altered(True)\n",
    "#pyplot.imshow(qq.load_image_pair(4)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq.run(clear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq.match_altered(update=True)\n",
    "qq._view_sample_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation with Nifti loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "# https://nipy.org/nibabel/gettingstarted.html\n",
    "\n",
    "#nibabel load gets the shape and data type without needing to load anything. To do that you need to call get_fdata()\n",
    "img = nib.load('../data/nifti/MPRAGE_Structural_Scan.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get information from the header of img\n",
    "img.header.get_xyzt_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "data = img.get_fdata()\n",
    "\n",
    "# dimensions are [coronal,axial,sagittal]\n",
    "#pyplot.imshow(warp(data[:,90,:],AffineTransform(rotation=6)))\n",
    "pyplot.imshow(rotate(data[:,:,70], 10))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to the data\n",
    "_ = rescale(data, scale=(2,2,2), mode='symmetric')\n",
    "pyplot.imshow(_[:,:,180])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation\n",
    "\n",
    "#okay, for some reason, scikitimage does not work with 3D as intended, but will accept it for some reason....\n",
    "trans = np.eye(3)\n",
    "trans[:2,2]=40 #move 40 in the coronal and axial directions (i think?)\n",
    "transform = AffineTransform(matrix = trans)\n",
    "_ = warp(data, transform, mode = 'reflect')\n",
    "pyplot.imshow(_[:,:,90])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going to need to normalize the nifti data into some consistent range...\n",
    "_.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotation\n",
    "samp_dat = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n",
    "#im_h = rotate(im_h, _a, mode=\"reflect\")\n",
    "rotate(samp_dat,angle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to do rotation in another way\n",
    "from math import sin, cos, radians\n",
    "trans = np.eye(3)\n",
    "theta = radians(20)\n",
    "trans[0,0]=cos(theta)\n",
    "trans[0,1]=-sin(theta)\n",
    "trans[1,0]=sin(theta)\n",
    "trans[1,1]=cos(theta)\n",
    "\n",
    "#For just rotation along the sagittal plane with no scaling:\n",
    "# [cos(), -sin(), 0]\n",
    "# [sin(), cos(), 0]\n",
    "# [0, 0, 1]\n",
    "\n",
    "transform = AffineTransform(matrix = trans)\n",
    "_ = warp(data, transform, mode = 'reflect')\n",
    "pyplot.imshow(_[:,:,90])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selection of nifty file as its own nifti file\n",
    "qq = nib.Nifti1Image(data[:,:,90], np.eye(4))\n",
    "nib.save(qq, './practice_nifti.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load('./practice_nifti.nii.gz') #test loading of nii.gz files\n",
    "data = img.get_fdata()\n",
    "pyplot.imshow(data)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation with DICOM loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as dicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load single dicom image\n",
    "ds = dicom.dcmread('../data/dicom/A0009')\n",
    "(2,) + ds.pixel_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stack of .dcm images\n",
    "image_path = '../data/dicom/'\n",
    "\n",
    "im_list = sorted(os.listdir(image_path)) #SORTING IS VERY IMPORTANT FOR 3D IMAGES IN DICOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk = np.zeros((len(im_list),)+dicom.dcmread(image_path+im_list[0]).pixel_array.shape) # weird thing I have to do to make (h,w) into (s, h, w) because you can add to tuples \n",
    "for idx, f in enumerate(im_list):\n",
    "    #print(f)\n",
    "    stk[idx,:,:] = dicom.dcmread(image_path + f).pixel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(stk[41,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definition V2.0\n",
    "With the previous version of the class done, and some lesson learned from actually implementing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "Loading ../data/SCN_png/Set14/Raw_train/barbara.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "918it [00:00, 8916.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 918\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "918it [00:00, 13304.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/face.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:00, 9560.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 144\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "144it [00:00, 12375.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/flowers.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "368it [00:00, 9447.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 368\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "368it [00:00, 13794.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/bridge.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [00:00, 13147.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 576\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [00:00, 17060.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/man.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [00:00, 9812.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 576\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [00:00, 10781.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/foreman.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [00:00, 7998.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 208\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [00:00, 10027.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/coastguard.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [00:00, 9108.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 208\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "208it [00:00, 11986.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/lenna.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [00:00, 8613.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 576\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576it [00:00, 11684.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/baboon.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "506it [00:00, 7137.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 506\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "506it [00:00, 10957.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/SCN_png/Set14/Raw_train/comic.png as png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:00, 5734.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 176\n",
      "Number of blank patches: 0\n",
      "keeping blank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:00, 9166.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import math as math\n",
    "import nibabel as nib\n",
    "import pydicom\n",
    "from skimage.transform import rotate, AffineTransform, warp, rescale, resize\n",
    "\n",
    "class SrGen:\n",
    "    def __init__(self, inp_dir, HR_out_dir, LR_out_dir, prefix='', suffix=''):\n",
    "        self.inp_dir = inp_dir\n",
    "        self.HR_out_dir = HR_out_dir\n",
    "        self.HR_files = None\n",
    "        self.LR_out_dir = LR_out_dir\n",
    "        self.LR_files = None\n",
    "        self.inp_files, self.inp_paths = self._get_inp_(prefix, suffix)\n",
    "        self.template = self.get_template()\n",
    "\n",
    "### ORANIZATIONAL METHODS\n",
    "    def _get_inp_(self, prefix='', suffix=''):\n",
    "\n",
    "        files = []\n",
    "        paths = []\n",
    "        # If they have provided a list of directories (in the case of DICOM or scattered data)\n",
    "        if isinstance(self.inp_dir, list):\n",
    "            for inp_dir in self.inp_dir:\n",
    "                for fil in os.listdir(inp_dir):\n",
    "                    if fil.startswith(prefix) and fil.endswith(suffix):\n",
    "                        paths.append(inp_dir + fil)\n",
    "                        files.append(fil)\n",
    "\n",
    "                    if not files:\n",
    "                        raise FileNotFoundError('No applicable files found in input directory')\n",
    "        else:\n",
    "            for fil in os.listdir(self.inp_dir):\n",
    "                if fil.startswith(prefix) and fil.endswith(suffix):\n",
    "                    paths.append(self.inp_dir + fil)\n",
    "                    files.append(fil)\n",
    "\n",
    "                if not files:\n",
    "                    raise FileNotFoundError('No applicable files found in input directory')\n",
    "\n",
    "        return files, paths\n",
    "    \n",
    "    def _get_LR_out_(self):\n",
    "        # get list of files in output directory and determine matching files\n",
    "        return os.listdir(self.LR_out_dir)\n",
    "\n",
    "    def _get_HR_out_(self):\n",
    "        return os.listdir(self.HR_out_dir)\n",
    "\n",
    "    def get_template(self):\n",
    "        try:\n",
    "            return self.template\n",
    "        except:\n",
    "            return {'out_type':'png',\n",
    "                'unit':'intensity', #Currently only matters for png\n",
    "                'resolution':None,\n",
    "                'same_size': True, # Whether to have the LR image be the same size as the HR image\n",
    "                                   # (i.e. whether to scale down then up or just down)\n",
    "                'translation':None, # Have both single value or multiple\n",
    "                'rotation': None, # Around each axis\n",
    "                'scale': False, # What magnitude to zoom in for added jitter\n",
    "                'patch': False, # Have this accept 3 dimensional input [x,y,z], [x,y], or single\n",
    "                'step': 10, # Also have this accept 3 dimensional input\n",
    "                'keep_blank': False,\n",
    "                'blank_ratio': 0.4,\n",
    "            }\n",
    "    \n",
    "    def set_template(self, temp):\n",
    "        self.template = temp\n",
    "\n",
    "\n",
    "    def match_altered(self, update=True, paths=False, sort=False):\n",
    "        # Get the files that have been generated in the output directory\n",
    "        # If update is false, then just return a list of matched names, if true then\n",
    "        # change the class variable values accordingly.\n",
    "        hr_files = self._get_HR_out_()\n",
    "        lr_files = self._get_LR_out_()\n",
    "\n",
    "        # Get a set of all the files with agreement before the metadata\n",
    "        if len(hr_files) > len(lr_files):\n",
    "            if sort: #TODO: make sort so it isnt [*1.*, *10.*, *100.*, ..., *2.*,...]\n",
    "                matches = sorted(list(set(hr_files)-(set(hr_files)-set(lr_files))))\n",
    "            else:\n",
    "                matches = list(set(hr_files)-(set(hr_files)-set(lr_files)))\n",
    "        else:\n",
    "            if sort:\n",
    "                matches = sorted(list(set(lr_files)-(set(lr_files)-set(hr_files))))\n",
    "            else:\n",
    "                matches = list(set(lr_files)-(set(lr_files)-set(hr_files)))\n",
    "\n",
    "        if update:\n",
    "            # If you want to save these matched files as class variables\n",
    "            self.HR_files = [self.HR_out_dir + _ for _ in matches]\n",
    "            self.LR_files = [self.LR_out_dir + _ for _ in matches]\n",
    "            print('HR and LR file locations updated')\n",
    "        \n",
    "        if paths:\n",
    "            return self.HR_files, self.LR_files\n",
    "\n",
    "    def change_out(self, HR_out_dir, LR_out_dir):\n",
    "        # Change the output locations so you can save into a new file\n",
    "        self.HR_out_dir = HR_out_dir\n",
    "        self.HR_files = None\n",
    "        self.LR_out_dir = LR_out_dir\n",
    "        self.LR_files = None\n",
    "\n",
    "### ACTUAL IMAGE MANIPULATION BELOW\n",
    "\n",
    "    def run(self, clear=False, save=False, verbose=False):\n",
    "        # This method is called to generate the data\n",
    "\n",
    "        if clear:\n",
    "            print('Clearing existing output directories')\n",
    "            shutil.rmtree(self.HR_out_dir, ignore_errors=True)\n",
    "            if self.template['resolution'] != None:\n",
    "                shutil.rmtree(self.LR_out_dir, ignore_errors=True)\n",
    "                os.makedirs(self.LR_out_dir, exist_ok=True)\n",
    "\n",
    "        os.makedirs(self.HR_out_dir, exist_ok=True)\n",
    "        fnames_h = []\n",
    "        fnames_l = []\n",
    "\n",
    "        for ids, im in enumerate(self.inp_paths):\n",
    "            im_h = self.load_image(im, verbose)\n",
    "            opp, im_h = self.img_transform(im_h)\n",
    "\n",
    "            # Prevents weird new file naming issues when input is compressed file (.nii.gz)\n",
    "            im = Path(im)\n",
    "            while im.suffix in {'.tar', '.gz', '.zip'}:\n",
    "                im = im.with_suffix('')\n",
    "            \n",
    "            im = os.path.splitext(im)[0]+opp # Add transformations to file name\n",
    "            im = os.path.split(im)[1]\n",
    "        \n",
    "            # Generate Low Resolution\n",
    "            if self.template['resolution']:\n",
    "\n",
    "                dim = im_h.shape\n",
    "                # efficient way to either make a single value into an array or do nothing if resolution is already a vector\n",
    "                # TODO: replace transformation if statements with this\n",
    "                self.template['resolution'] = [int(x) for x in np.multiply(np.ones(len(dim)), self.template['resolution'])]\n",
    "\n",
    "                # Check that dimensions of HR image are multiples of resolution change, else shave off data\n",
    "                for i in range(len(dim)):\n",
    "                    if dim[i] % self.template['resolution'][i]:\n",
    "                        # If it isn't a clean scaling down\n",
    "                        _ = dim[i]-(dim[i] % self.template['resolution'][i])\n",
    "\n",
    "                        im_h = np.delete(im_h,[x for x in range(_, dim[i])],i)\n",
    "\n",
    "                im_l = self.gen_LR_img(im_h, self.template['resolution'])\n",
    "\n",
    "            # Create image patches and save them\n",
    "            if self.template['patch'] and save:\n",
    "                fnames_h, slice_select = self.img2patches(im_h, self.HR_out_dir + im, save=True, sanity_check=True)\n",
    "                if self.template['resolution'] != None:\n",
    "                    fnames_l = self.img2patches(im_l, self.LR_out_dir + im, same_size=self.template['same_size'], save=True, slice_select=slice_select, sanity_check=False)\n",
    "\n",
    "                    # if not _a == _:\n",
    "                    #     raise FileExistsError('''WARNING: The patches for High and Low resolution do not match, this is\n",
    "                    #         most likely due to resolution scaling or patches/steps not being divisible by resolution''')\n",
    "\n",
    "            elif save:\n",
    "                fname_h = self.HR_out_dir + im\n",
    "                self.save_image(fname_h, im_h, verbose)\n",
    "                fnames_h.append(fname_h)\n",
    "                if self.template['resolution']:# != None:\n",
    "                    fname_l = self.LR_out_dir + im\n",
    "                    self.save_image(fname_l, im_l, verbose)\n",
    "                    fnames_l.append(fname_l)\n",
    "\n",
    "\n",
    "        self.HR_files = fnames_h\n",
    "        self.LR_files = fnames_l\n",
    "\n",
    "        print('Files processed successfully')\n",
    "\n",
    "\n",
    "\n",
    "    def load_image(self, im_path, verbose=False):\n",
    "        # Given an image path, determines the function required to load the contents\n",
    "        # as a numpy array, which is returned.\n",
    "        fil_typ = os.path.splitext(im_path)[1]\n",
    "\n",
    "        if fil_typ == '.png':\n",
    "            # If file is a png\n",
    "            with Image.open(im_path) as f_im:\n",
    "                img = np.array(f_im)\n",
    "\n",
    "            print(f'Loading {im_path} as png')\n",
    "            if verbose:\n",
    "                print(f'Image shape:{img.shape}')\n",
    "\n",
    "            if self.template['unit'] == 'intensity':\n",
    "                if len(img.shape)==3:\n",
    "                    img = self.rgb2ycrbcr(img)\n",
    "                    img = img[:,:,0] #Just deal with intensity values at the moment because \n",
    "                                    # having multiple channels throws off cv2 when saving, \n",
    "                                    # since it also does BGR instead of RGB and will save a blue image\n",
    "                elif len(img.shape)==2:\n",
    "                    pass            # If the png is just greyscale, then there is nothing that can\n",
    "                                    # be done except take the single channel\n",
    "                else:\n",
    "                    raise ImportError(\"Provided png image is not 2 or 3 dimensional, something is wrong with the image.\")\n",
    "\n",
    "            elif self.template['unit'] == 'color':\n",
    "                raise NotImplementedError(\"\"\"Loading and creation of patches from color png images is\n",
    "                currently not supported. Please use template['unit']='intensity' for conversion of png\n",
    "                imges to greyscale intesity images.\"\"\")\n",
    "\n",
    "        elif fil_typ == '.nii' or fil_typ == '.gz':\n",
    "            img = nib.load(im_path).get_fdata()\n",
    "            if verbose:\n",
    "                print(f'Loading {im_path} as nii')\n",
    "                print(f'Image shape:{img.shape}')\n",
    "\n",
    "        elif fil_typ == '.dcm':\n",
    "            img = pydicom.dcmread(im_path).pixel_array\n",
    "            if verbose:\n",
    "                print(f'Loading {im_path} as dicom')\n",
    "                print(f'Image shape:{img.shape}')\n",
    "\n",
    "        else:\n",
    "            raise FileNotFoundError(f'Image file type {fil_typ} not supported.')\n",
    "\n",
    "        return img\n",
    "\n",
    "    def gen_LR_img(self, im_h, res, interp=1):\n",
    "        # Generate the low-resolution image from the corresponding HR image using resizing\n",
    "        dim = im_h.shape\n",
    "\n",
    "        # TODO: Patching error occurs when the shape of an image has dimension of odd magintude with\n",
    "        #       even 'res', and vice versa. Need to come up with a fix for this...\n",
    "        new_dims = [math.floor(x) for x in np.divide(dim, res)]\n",
    "\n",
    "        im_l = resize(im_h, new_dims, order = interp, mode='symmetric')\n",
    "\n",
    "        if self.template['same_size']:\n",
    "            im_l = resize(im_l, dim, order= interp, mode = 'symmetric')\n",
    "\n",
    "        return im_l\n",
    "\n",
    "    def img_transform(self, im_h, save=False):\n",
    "        # Transform the original files using a variety of methods\n",
    "        # Have im_h and im_l be numpy arrays\n",
    "        opp = '' #string for storing the operations performed on the images\n",
    "\n",
    "        dim = im_h.shape\n",
    "        if len(dim)>3:\n",
    "            raise ValueError('Dimension of input data not currently supported')\n",
    "        \n",
    "        # If single image is provided for any of these settings, convert into list of N dimensions\n",
    "        if self.template['translation'] == None:\n",
    "            trans = [None]\n",
    "        elif type(self.template['translation']) != list:\n",
    "            trans = [self.template['translation'] for _ in range(len(dim))]\n",
    "        else:\n",
    "            trans = self.template['translation'][:] #Weird thing I have to add to not link changes to 'trans' to self.template\n",
    "        \n",
    "        for idx, x in enumerate(trans):\n",
    "            if trans[idx] != 0 and trans[idx] != None:\n",
    "                trans[idx] = np.random.randint(-x,x)\n",
    "\n",
    "\n",
    "        # Rotation\n",
    "        if self.template['rotation'] == None:\n",
    "            rot = [None]\n",
    "        elif type(self.template['rotation']) != list:\n",
    "            rot = [self.template['rotation'] for _ in range(len(dim))]\n",
    "        else:\n",
    "            rot = self.template['rotation'][:]\n",
    "\n",
    "       \n",
    "        for idx, x in enumerate(rot):\n",
    "            if x != 0 and x != None:\n",
    "                rot[idx] = np.random.randint(-x,x)\n",
    "\n",
    "\n",
    "        # Scaling\n",
    "        if self.template['scale'] == None:\n",
    "            scale = [None]\n",
    "        elif type(self.template['scale']) != list:\n",
    "            scale = [self.template['scale'] for _ in range(len(dim))]\n",
    "        else:\n",
    "            scale = self.template['scale'][:]\n",
    "\n",
    "        for idx, x in enumerate(scale):\n",
    "            if scale[idx] != None and scale[idx] > 1:\n",
    "                scale[idx] = np.random.randint(1,x+1)\n",
    "            else:\n",
    "                scale[idx] = 1\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Issue with low resolution not necessairly having the same dimensions\n",
    "        # Rotation 2D\n",
    "        if len(rot) == 2 and type(rot[0])==int: #Make sure rotation isn't None for some reason\n",
    "            im_h = rotate(im_h, rot[0], order=1)\n",
    "            opp += f'_rot{rot[0]}'\n",
    "        # Rotation 3D\n",
    "        elif len(rot) == 3:\n",
    "            for i in range(dim[0]):\n",
    "                im_h[i,:,:] = rotate(im_h[i,:,:],rot[0], order=1)\n",
    "            for i in range(dim[1]):\n",
    "                im_h[:,i,:] = rotate(im_h[:,i,:],rot[1], order=1)\n",
    "            for i in range(dim[2]):\n",
    "                im_h[:,:,i] = rotate(im_h[:,:,i],rot[2], order=1)\n",
    "            opp += f'_rot{rot[0]}_{rot[1]}_{rot[2]}'\n",
    "            \n",
    "\n",
    "        # Translation\n",
    "        if len(trans) == 2:\n",
    "            transform = AffineTransform(translation=(trans[0], trans[1]))\n",
    "            im_h = warp(im_h, transform, mode=\"symmetric\")\n",
    "            opp += f'_tr{trans[0]}_{trans[1]}'\n",
    "        elif len(trans) == 3:\n",
    "            transform = AffineTransform(translation=(trans[1], trans[2]))\n",
    "            for i in range(dim[0]):\n",
    "                im_h[i,:,:] = warp(im_h[i,:,:],transform, mode = 'symmetric')\n",
    "\n",
    "            for i in range(dim[1]):\n",
    "                # Because two dimensions were already translated, you only need to translate\n",
    "                # along one dimension\n",
    "                im_h[:,i,:] = warp(im_h[:,i,:], AffineTransform(translation=(trans[0],0)), mode='symmetric')\n",
    "                \n",
    "            opp += f'_tr{trans[0]}_{trans[1]}_{trans[2]}'\n",
    "\n",
    "        # Scaling\n",
    "        if len(scale) >= 2:\n",
    "            #transform = AffineTransform(scale=_a)\n",
    "            im_h = rescale(im_h, scale = scale, mode='symmetric')\n",
    "            \n",
    "            # Dumb way to make added string fit based on scaling\n",
    "            try: opp+= f'_scale_{scale[0]}_{scale[1]}_{scale[2]}'\n",
    "            except: opp+= f'_scale_{scale[0]}_{scale[1]}'\n",
    "\n",
    "        return opp, im_h\n",
    "\n",
    "    def img2patches(self, im_h, fname, same_size=True, keep_blank=False, slice_select=None, save=False, sanity_check=False, verbose=False):\n",
    "        # Depending on the number of dimenions in the `patch` value, either make 2D\n",
    "        # or 3D images\n",
    "\n",
    "        dim = im_h.shape\n",
    "        patch_size = self.template['patch'][:]\n",
    "        step = self.template['step'][:]\n",
    "\n",
    "        \n",
    "        im_name=Path(fname).with_suffix('').__str__()\n",
    "        #im_name = fname.split('.')[:-2][0] #Kind of janky way to just strip away the suffix\n",
    "        \n",
    "        if slice_select: #If slice_select is provided, then getting rid of blanks really screws things up\n",
    "            print('keeping blank')\n",
    "            keep_blank = True\n",
    "\n",
    "        if type(patch_size) != list:\n",
    "            patch_size = [patch_size for _ in range(len(dim))]\n",
    "        \n",
    "        if type(step) != list:\n",
    "            step = [step for _ in range(len(dim))]\n",
    "\n",
    "        # Whether to shrink the patch size and step size down by the scaling amount for LR images without the same dimensions as the HR images\n",
    "        if not same_size:\n",
    "            try: \n",
    "                patch_size = [math.floor(x) for x in np.divide(patch_size,self.template['resolution'])]\n",
    "            except: \n",
    "                raise ValueError(f'Resolution change coefficient: {self.template[\"resolution\"]} not defined properly for patch_size: {patch_size}')\n",
    "\n",
    "            try: \n",
    "                step = [math.floor(x) for x in np.divide(step,self.template['resolution'])]\n",
    "            except:\n",
    "                raise ValueError(f'Resolution change coefficient: {self.template[\"resolution\"]} not defined properly for step: {step}')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f'patch size = {patch_size}')\n",
    "                print(f'step size = {step}')\n",
    "\n",
    "        # Create a numpy stack following Pytorch protocols, so 1 dimension more than patch\n",
    "        \n",
    "        # Count number of non-zero entries\n",
    "        cnt = 0\n",
    "        blank = 0\n",
    "        not_blank = []\n",
    "        itter = -1\n",
    "\n",
    "        # Get total number of patches that will be created:\n",
    "        #patch_count = np.prod([len(range(0,i,step[idx])) for idx, i in enumerate(dim)])\n",
    "        if verbose:\n",
    "            print(f'patch guess = {np.prod([math.floor((i-patch_size[idx])/step[idx])+1 for idx,i in enumerate(dim)])}')\n",
    "        patch_count = np.prod([math.floor((i-patch_size[idx])/step[idx])+1 for idx,i in enumerate(dim)])\n",
    "        patch_vol = math.prod(patch_size)*self.template['blank_ratio']\n",
    "\n",
    "        if len(dim) == 2:\n",
    "            stack = np.zeros((patch_count,patch_size[0],patch_size[1]))\n",
    "            if verbose:\n",
    "                print(f'stack size = {stack.shape}')\n",
    "\n",
    "            for i in range(0,dim[0],step[0]):\n",
    "                for j in range(0,dim[1],step[1]):\n",
    "                    if i+patch_size[0] <= dim[0] and j+patch_size[1] <= dim[1]:\n",
    "                        itter = itter+1 #just a calculator for finding when blanks occur\n",
    "                        samp = im_h[i:i+patch_size[0],j:j+patch_size[1]]\n",
    "\n",
    "                        if keep_blank or (samp==0).sum() <= patch_vol:#(samp.max() > 0):\n",
    "                            stack[cnt,:,:] = samp\n",
    "                            cnt += 1\n",
    "                            not_blank.append(itter)\n",
    "                        else:\n",
    "                            blank += 1\n",
    "                            #blank.append(_)\n",
    "        elif len(dim) == 3:\n",
    "            stack = np.zeros((patch_count,patch_size[0],patch_size[1], patch_size[2]))\n",
    "            print(f'stack size = {stack.shape}')\n",
    "\n",
    "            for i in range(0,dim[0],step[0]):\n",
    "                for j in range(0,dim[1],step[1]):\n",
    "                    for k in range(0,dim[2],step[2]):\n",
    "                        #itter = itter+1 #just a calculator for finding when blanks occur\n",
    "                        if i+patch_size[0] <= dim[0] and j+patch_size[1] <= dim[1] and k+patch_size[2] <= dim[2]:\n",
    "                            itter = itter+1\n",
    "                            samp = im_h[i:i+patch_size[0],j:j+patch_size[1], k:k+patch_size[2]]\n",
    "\n",
    "                            if keep_blank or (samp==0).sum() <= patch_vol:#(samp.max() > 0):\n",
    "                                stack[cnt,:,:,:] = samp\n",
    "                                cnt += 1\n",
    "                                not_blank.append(itter)\n",
    "                            else:\n",
    "                                blank += 1\n",
    "                                #blank.append(_)\n",
    "            print(itter)\n",
    "        else:\n",
    "            raise IndexError(f'Images of dimension {dim} not supported by this method. Only 2D and 3D data accepted.')\n",
    "        \n",
    "\n",
    "        #TODO: There MUST be a better way to organize this whole mess, lol\n",
    "\n",
    "        fnames = []\n",
    "        if slice_select:\n",
    "            for i in range(len(slice_select)):\n",
    "                fnames.append(f'{im_name}_{i}.{self.template[\"out_type\"]}')\n",
    "        else:\n",
    "            for i in range(cnt):\n",
    "                fnames.append(f'{im_name}_{i}.{self.template[\"out_type\"]}')\n",
    "\n",
    "        if save:\n",
    "            if slice_select:\n",
    "                for idx, i in tqdm(enumerate(slice_select)):\n",
    "                    self.save_image(fnames[idx], stack[i], verbose)\n",
    "            else:\n",
    "                for idx, i in tqdm(enumerate(fnames)):\n",
    "                    self.save_image(i,stack[idx], verbose)\n",
    "            if sanity_check:\n",
    "                print(f'Number of patches: {len(not_blank)}')\n",
    "                print(f'Number of blank patches: {blank}')\n",
    "                return fnames, not_blank\n",
    "            else:\n",
    "                return fnames\n",
    "        else:\n",
    "            if sanity_check:\n",
    "                return fnames, stack, not_blank\n",
    "            else:\n",
    "                return fnames, stack\n",
    "\n",
    "    def rgb2ycrbcr(self, img_rgb):\n",
    "        #Takes an RBG image and returns it as a YCRBCR image \n",
    "        # (if you just want to focus on luminance values of an image)\n",
    "\n",
    "        img_rgb = img_rgb.astype(np.float32)\n",
    "        \n",
    "        img_ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2YCR_CB)\n",
    "        img_ycbcr = img_ycrcb[:,:,(0,2,1)].astype(np.float32)\n",
    "        img_ycbcr[:,:,0] = (img_ycbcr[:,:,0]*(235-16)+16)/255.0\n",
    "        img_ycbcr[:,:,1:] = (img_ycbcr[:,:,1:]*(240-16)+16)/255.0\n",
    "\n",
    "        return img_ycbcr\n",
    "\n",
    "    def load_image_pair(self, im_id):\n",
    "        # A method which loads the provided file and returns a numpy array\n",
    "        # this is used because it will remember in the template dictionary how\n",
    "        # the images were saved (either as RBG or intensity values or 3D array). \n",
    "        # This will help minimize headaches caused by different image types.\n",
    "\n",
    "        # im_id can either be the index value or the name of the file\n",
    "        if isinstance(im_id, int):\n",
    "            HR_file = self.HR_files[im_id]\n",
    "            LR_file = self.LR_files[im_id]\n",
    "        elif isinstance(im_id, str):\n",
    "             _ = self.HR_files.index(im_id)\n",
    "             HR_file = self.HR_files[_]\n",
    "             LR_file = self.LR_files[_]\n",
    "        else:\n",
    "            TypeError(\"Invalid image identifier, please input a string to integer\")\n",
    "\n",
    "        im_h = self.load_image(HR_file)\n",
    "        im_l = self.load_image(LR_file)\n",
    "\n",
    "        return im_h, im_l\n",
    "\n",
    "\n",
    "    def save_image(self, fname, im, form=None, verbose = False):\n",
    "        # Take a given image and save it as the specified format:\n",
    "        # fname = output name of the saved file\n",
    "        # im = numpy array of image\n",
    "\n",
    "        if not form:\n",
    "            form = self.template['out_type']\n",
    "\n",
    "        dim = im.shape #Get number of dimensions of image\n",
    "\n",
    "        if form == 'png':\n",
    "            # Check that you aren't saving a 3D image\n",
    "            #TODO: Scale inputs to [0,255] so data isn't lost/image isn't saturated\n",
    "            cv2.imwrite(f'{fname}',im)\n",
    "            if verbose:\n",
    "                print(f'Saving: {fname}')\n",
    "        elif form == 'nii':\n",
    "\n",
    "            # TODO: Add option to transpose image for some reason because mricron hates the first dim[0] = 1\n",
    "            # Still gets loaded fine in terms of loading into python, but visualizing it is bad\n",
    "            # np.transpose(im, (1,2,0))\n",
    "\n",
    "\n",
    "            # TODO: If image is 2D then append a third  dimension before saving(?)\n",
    "            nib.save(nib.Nifti1Image(im, np.eye(len(dim)+1)), fname)\n",
    "            if verbose:\n",
    "                print(f'Saving: {fname}')\n",
    "        elif form == 'dcm':\n",
    "            raise NotImplementedError('DICOM saving currently not supported')\n",
    "        else:\n",
    "            raise NotImplementedError('Specified file type is currently not supported for saving')\n",
    "\n",
    "\n",
    "# testclass = SrGen('../data/CNNIL_nifti/Raw/','../data/CNNIL_nifti/HR_patches_2/','../data/CNNIL_nifti/LR_patches_2/')\n",
    "# temp = testclass.get_template()\n",
    "# temp['out_type'] = 'nii'\n",
    "# temp['resolution'] = [1,2,2]\n",
    "# temp['translation'] = [0, 0, 0]\n",
    "# temp['rotation'] = [0, 0, 0]\n",
    "# # temp['scale']= [1,1,1]\n",
    "# temp['keep_blank'] = False\n",
    "# temp['same_size'] = False\n",
    "# temp['patch'] = [1,14,14] #[x,y,z] when looking at the brain from the top down\n",
    "# temp['step'] = [2,10,10]\n",
    "# testclass.set_template(temp)\n",
    "\n",
    "# # testclass.run(clear=True, save=True)\n",
    "\n",
    "sr_train = SrGen('../data/SCN_png/Set14/Raw_train/','../data/SCN_png/Set14/HR_train_patches/',\n",
    "        '../data/SCN_png/Set14/LR_train_patches/')\n",
    "\n",
    "temp = sr_train.get_template()\n",
    "temp[\"out_type\"] = 'png'\n",
    "temp[\"resolution\"] = [2,2]\n",
    "temp[\"patch\"]=[44,44]\n",
    "temp[\"step\"]=[20,20]\n",
    "temp[\"translation\"]=[10,10]\n",
    "temp[\"rotation\"] = 0\n",
    "temp[\"scale\"] = False\n",
    "temp[\"same_size\"] = False\n",
    "temp[\"unit\"] = 'intensity'\n",
    "\n",
    "sr_train.set_template(temp)\n",
    "\n",
    "sr_train.run(clear=True, save=True, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = np.array([2,3,4])\n",
    "[_ for _ in range(int(q[0]-1),int(q[2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR and LR file locations updated\n",
      "14699 HR files and 14699 LR files\n"
     ]
    }
   ],
   "source": [
    "a,b = testclass.match_altered(update=True, paths=True, sort=True)\n",
    "print(f'{len(a)} HR files and {len(b)} LR files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/CNNIL_nifti/HR_patches_2/29006_anat_rot0_0_0_tr0_0_0_scale_1_1_1_10.nii as nii\n",
      "Loading ../data/CNNIL_nifti/LR_patches_2/29006_anat_rot0_0_0_tr0_0_0_scale_1_1_1_10.nii as nii\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a,b = testclass.load_image_pair(2)\n",
    "pyplot.imshow(a[0,:,:])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(b[0,:,:])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/CNNIL_nifti/HR_patches_ax/29006_anat_rot0_0_0_tr1_0_8_scale_1_1_1_11.nii as nii\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(testclass.load_image('../data/CNNIL_nifti/HR_patches_ax/29006_anat_rot0_0_0_tr1_0_8_scale_1_1_1_11.nii'),(2,0,1))\n",
    "# testclass.load_image('../data/CNNIL_nifti/HR_patches_ax/29006_anat_rot0_0_0_tr1_0_8_scale_1_1_1_11.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1,1):\n",
    "    print(i)\n",
    "# if not a == b:\n",
    "#     raise FileExistsError('''WARNING: The patches for High and Low resolution do not match,\n",
    "#         most likely due to resolution scaling or patches/steps not being divisible by resolution''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.    , 2.25  , 0.75  , 0.    ],\n",
       "       [2.75  , 2.0625, 0.6875, 0.    ],\n",
       "       [2.25  , 1.6875, 0.5625, 0.    ],\n",
       "       [2.    , 1.5   , 0.5   , 0.    ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = np.zeros((2,2))\n",
    "a[1,0]=2\n",
    "a[0,0]=3\n",
    "resize(a,(4,4), order=1, mode='symmetric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n"
     ]
    }
   ],
   "source": [
    "if True or (a.max() >0):\n",
    "    print('this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/this/right/here'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = Path('/this/right/here')\n",
    "filename.with_suffix('').__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Dataloader and NN model\n",
    "Here we'll make sure the above class works in a way that is compatible with Pytorch's Dataloader and ML modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: There's not reasing I can't combine the Dataset class and my custom class into one thing\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sr_class):\n",
    "        self.sr_class = sr_class\n",
    "\n",
    "        # In case I forget to run match_altered before pulling the class\n",
    "        if not sr_class.HR_files:\n",
    "            sr_class.match_altered(update=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sr_class.HR_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        Y, X = self.sr_class.load_image_pair(index)\n",
    "        X = torch.unsqueeze(torch.tensor(X, dtype=torch.float32),0)\n",
    "        Y = torch.unsqueeze(torch.tensor(Y, dtype=torch.float32),0)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "training_set = Dataset(qq)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCN(nn.Module):\n",
    "    def __init__(self,sy,sg, model_file=False, train=True):\n",
    "        super().__init__()\n",
    "        C = 5\n",
    "        L = 5\n",
    "\n",
    "        Dx = torch.normal(0,1, size = (25,128))\n",
    "        Dy = torch.normal(0,1, size = (100,128))\n",
    "        I = torch.eye(128)\n",
    "\n",
    "        self.conv = nn.Conv2d(1,100,9, bias = False, stride =1, padding = 6)\n",
    "        self.mean2 = nn.Conv2d(1,1,13, bias = False, stride = 1, padding = 6)\n",
    "        self.diffms = nn.Conv2d(1,25,9, bias=False, stride = 1, padding=6)\n",
    "\n",
    "        self.wd = nn.Conv2d(100,128,1,bias = False, stride = 1)\n",
    "        self.usd1 = nn.Conv2d(128, 128, 1, bias = False, stride=1)\n",
    "        self.ud = nn.Conv2d(128,25,1,bias=False,stride=1)\n",
    "        self.addp = nn.Conv2d(16,1,1, bias = False, stride = 1)\n",
    "\n",
    "        if train: #If you are currently training the model\n",
    "            self.mean2.weight = torch.nn.Parameter(self.create_gaus(13), requires_grad = False)\n",
    "            self.diffms.weight = torch.nn.Parameter(self.create_diffms(9,5),requires_grad=False)\n",
    "            self.wd.weight = torch.nn.Parameter(self.expand_params(C*Dy.T), requires_grad=True)\n",
    "            self.usd1.weight = torch.nn.Parameter(self.expand_params(I - torch.matmul(Dy.T,Dy)), requires_grad=True)\n",
    "            self.ud.weight = torch.nn.Parameter(self.expand_params((1/(C*L))*Dx), requires_grad=True)\n",
    "            self.addp.weight = torch.nn.Parameter(torch.ones(1,16,1,1)*0.06, requires_grad=True)\n",
    "\n",
    "        else:\n",
    "            self.conv.weight = torch.nn.Parameter(torch.ones(100,1,9,9),requires_grad=False)\n",
    "            self.mean2.weight = torch.nn.Parameter(self.create_gaus(13),requires_grad=False)\n",
    "            self.diffms.weight = torch.nn.Parameter(self.create_diffms(9,5),requires_grad=False)\n",
    "            self.wd.weight = torch.nn.Parameter(self.expand_params(C*Dy.T),requires_grad=False)\n",
    "            self.usd1.weight = torch.nn.Parameter(self.expand_params(I - torch.matmul(Dy.T,Dy)),requires_grad=False)\n",
    "            self.ud.weight = torch.nn.Parameter(self.expand_params((1/(C*L))*Dx),requires_grad=False)\n",
    "            self.addp.weight = torch.nn.Parameter(torch.ones(1,16,1,1)*0.06,requires_grad=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, k, sy=9, sg=5):\n",
    "        im_mean = self.mean2(x)\n",
    "        # print(f'im_mean shape {im_mean.shape}')\n",
    "        diffms = self.diffms(x)\n",
    "        # print(f'diffms shape: {diffms.shape}')\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "        # y = torch.zeros(n, 100, h-8, w-8)\n",
    "        x = self.conv(x)\n",
    "        # print(f'post conv shape {x.shape}')\n",
    "        #print(f'conv max {x.max()}')\n",
    "        x=x+1\n",
    "\n",
    "        x = x/torch.linalg.vector_norm(x, ord=2, dim=1, keepdim=True)\n",
    "        # print(f'post vector norm shape: {x.shape}')\n",
    "        #print(f'postnorm max {x.max()}')\n",
    "\n",
    "        x = self.wd(x)\n",
    "        #print(f'conv wd {x.max()}')\n",
    "        z = self.ShLU(x,1)\n",
    "        #print(f'conv SHLU {x.max()}')\n",
    "\n",
    "        # Go through LISTA\n",
    "        for i in range(k):\n",
    "            z = self.ShLU(self.usd1(z)+x,1)\n",
    "\n",
    "        x = self.ud(z)\n",
    "        #print(f'ud max {x.max()}')\n",
    "        # print(f'post ud shape {x.shape}')\n",
    "        x = (x/torch.linalg.vector_norm(x, ord=2, dim=1, keepdim=True))*torch.linalg.vector_norm(diffms, ord=2, dim=1, keepdim=True)*1.1\n",
    "        # print(f'prereassembled x shape {x.shape}')\n",
    "        x = self.reassemble2(x,im_mean,4)\n",
    "        # print(f'reassembled x shape {x.shape}')\n",
    "        x = self.addp(x)\n",
    "        #print(f'x.reassemble.max = {x.max()}')\n",
    "        x = x+im_mean\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reassemble2(self, x, im_mean, patch_size):\n",
    "        img = im_mean\n",
    "        s, c, h, w = img.shape\n",
    "        \n",
    "        # img_stack=torch.zeros(s,25,h,w)\n",
    "        img_stack=torch.zeros(s,16,h,w)\n",
    "        \n",
    "        #go through every sample and reassemble the image\n",
    "        for q in range(x.shape[0]):\n",
    "            filt = 0\n",
    "            for ii in range(patch_size-1, -1, -1):\n",
    "                for jj in range(patch_size-1, -1, -1):\n",
    "                    img_stack[q,filt,:,:] = x[q,filt,jj:(jj+h), ii:(ii+w)]\n",
    "                    filt+=1\n",
    "        \n",
    "        return img_stack\n",
    "    \n",
    "    def create_diffms(self, kern_size, sy=5):\n",
    "        diffms = torch.zeros(sy**2,1,kern_size,kern_size)\n",
    "        \n",
    "        neg = -1*(1/(sy**2))\n",
    "        pos = 1+neg\n",
    "        \n",
    "        border = int((kern_size-sy)/2)\n",
    "        base = torch.zeros(sy,sy)+neg\n",
    "        cnt=0\n",
    "        \n",
    "        for i in range(sy**2):\n",
    "            base = torch.zeros(sy**2)+neg\n",
    "            base[cnt]=pos\n",
    "            diffms[i,0,border:(kern_size-border),border:(kern_size-border)] = base.reshape([sy,sy])\n",
    "            cnt+=1\n",
    "        return diffms\n",
    "    \n",
    "    \n",
    "    def create_gaus(self, kern_size, sy=9,std=2.15):\n",
    "        n = torch.arange(0,sy)-(sy-1.0)/2.0\n",
    "        sig2 = 2 * std * std\n",
    "        gkern1d = torch.exp(-n ** 2 / sig2)\n",
    "        gkern1d = gkern1d/torch.sum(gkern1d)\n",
    "        #print(gkern1d.shape)\n",
    "        gkern2d = torch.outer(gkern1d, gkern1d)\n",
    "    \n",
    "\n",
    "        # Wrap in zeros, if kern_size > sy\n",
    "        gaussian_filter = torch.zeros(1,1,kern_size,kern_size)\n",
    "        border = int((kern_size-sy)/2)\n",
    "        gaussian_filter[0,0,border:(kern_size-border),border:(kern_size-border)] = gkern2d#(sy,std=std)\n",
    "        #print(gaussian_filter.shape)\n",
    "        return gaussian_filter\n",
    "        \n",
    "    \n",
    "    def fixed_positions(self, tens, mult, sg):\n",
    "        f, _ , h, w = tens.shape\n",
    "        new_filt = torch.zeros(f*mult, 1, sg,sg)\n",
    "        cnt = 0\n",
    "        filt = 0\n",
    "        \n",
    "        for filt in range(f):\n",
    "            for j in range((sg-w)+1):\n",
    "                for i in range((sg-h)+1):\n",
    "                    new_filt[cnt,0,i:i+h,j:j+w] = tens[filt]\n",
    "                    cnt+=1\n",
    "        return new_filt\n",
    "    \n",
    "    def expand_params(self,tens):\n",
    "        return torch.unsqueeze(torch.unsqueeze(tens,2),3)\n",
    "    \n",
    "    def ShLU(self,a, th):\n",
    "        return torch.sign(a)*torch.maximum(abs(a)-th, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SCN(9,5,train=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        {\"params\": net.addp.parameters()},#, \"lr\": 0.0002, \"momentum\": 0.00005},\n",
    "        {\"params\": net.conv.parameters()},#, \"lr\": 0.0003, \"momentum\": 0.0001},\n",
    "        {\"params\": net.wd.parameters()},\n",
    "        {\"params\": net.usd1.parameters()},\n",
    "        {\"params\": net.ud.parameters()},\n",
    "    ],\n",
    "    lr=0.00007, momentum = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "# Loop over epochs\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    losses = []\n",
    "    losses_per = []\n",
    "\n",
    "    # Training\n",
    "    count = 0\n",
    "    for inp, goal in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(inp,2) # the 2 is the number of iterations in the LISTA network\n",
    "        print(output.shape)\n",
    "        output = torch.clamp(output, 0, 255)\n",
    "\n",
    "        loss = criterion(output,goal)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'loss = {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "        print(f'mini-batch # {count}, mean loss = {sum(losses)/len(losses)}')\n",
    "        count = count+1\n",
    "\n",
    "    torch.save(net.state_dict(), f'./MRI_save_{epoch}.p')\n",
    "    print(f'\\n\\n epoch {epoch}, loss mean: {sum(losses)/len(losses)}, loss: {min(losses)}-{max(losses)}\\n')\n",
    "\n",
    "    # Give computer time to cool down\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539b544e2c3fdc58492248d082a132f5e0b4fea63e914fb274c32873997cf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
