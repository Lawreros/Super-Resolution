{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a replication/exploration of the paper listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..') # Stupid thing Python makes you do to import from a sibling directory\n",
    "from gen_utils.SrGen import SrGen # Custom class for image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNIL(nn.Module):\n",
    "    def __init__(self, upscale=2, axs = 'hw'):\n",
    "        super().__init__()\n",
    "        self.axs = axs\n",
    "\n",
    "        self.rel = nn.ReLU()\n",
    "        if axs == 'hw':\n",
    "            self.conv1 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv2 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv3 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv4 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv5 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv6 = nn.Conv2d(32,4,3, padding='same', bias=False)\n",
    "            \n",
    "            # Upscale step occurs here\n",
    "\n",
    "            self.conv7 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv8 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv9 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv10 = nn.Conv2d(32,1,3, padding='same', bias = False)\n",
    "        elif axs == 'h' or axs == 'w':\n",
    "            self.conv1 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv2 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv3 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv4 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv5 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv6 = nn.Conv2d(32,2,3, padding='same', bias=False)\n",
    "            \n",
    "            # Upscale step occurs here\n",
    "\n",
    "            self.conv7 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv8 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv9 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv10 = nn.Conv2d(32,1,3, padding='same', bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.rel(self.conv1(x))\n",
    "        \n",
    "        x_l = self.rel(self.conv2(x))\n",
    "        x_l = self.rel(self.conv3(x_l))\n",
    "        x_l = self.rel(self.conv4(x_l+x))\n",
    "        x_l = self.rel(self.conv5(x_l))\n",
    "        x_l = self.rel(self.conv6(x_l+x))\n",
    "\n",
    "        x_l = self.kern_upscale(x_l, axs = self.axs)\n",
    "\n",
    "        x = self.rel(self.conv7(x_l))\n",
    "        x_h = self.rel(self.conv8(x))\n",
    "        x_h = self.rel(self.conv9(x_h))\n",
    "        x_h = self.rel(self.conv10(x_h+x))\n",
    "\n",
    "        return x_l, x_h #Return both results for the loss function\n",
    "\n",
    "    @staticmethod\n",
    "    def kern_upscale(x, axs='hw'):\n",
    "        # Function to do the unique upscaling pattern they propose in\n",
    "        # the paper\n",
    "        #TODO: have them input a tuple for scale of the dimensions they wish to expand along\n",
    "        s, c, h, w = [int(_) for _ in list(x.shape)]\n",
    "\n",
    "        if axs == 'hw':\n",
    "            c = int(c/2)\n",
    "            x_up = torch.cat(torch.unbind(x,1),2)\n",
    "            x_up = torch.reshape(x_up,(s,1,h*c,w*c))\n",
    "            x_up = torch.transpose(x_up,2,3)\n",
    "            x_up = torch.cat(torch.split(x_up,2,2),3)\n",
    "            x_up = torch.transpose(torch.reshape(x_up,(s,1,h*c,w*c)),2,3)\n",
    "        elif axs == 'h':\n",
    "            x_up = torch.unsqueeze(torch.cat(torch.unbind(x,2),1),1)\n",
    "            #x_up = torch.unsqueeze(x_up,0)\n",
    "        elif axs == 'w':\n",
    "            x_up = torch.cat(torch.unbind(x,3),1)\n",
    "            x_up = torch.unsqueeze(torch.transpose(x_up,1,2),1)\n",
    "            #x_up = torch.unsqueeze(x_up,0)\n",
    "        else:\n",
    "            print('No valid scaling dimension selected, returning False')\n",
    "            x_up = False\n",
    "\n",
    "        return x_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing of kern_upscale function with only 3D tensor\n",
    "\n",
    "a = torch.tensor([[[1,2],[3,4]],\n",
    "                [[5,6],[7,8]],\n",
    "                [[9,10],[11,12]],\n",
    "                [[13,14],[15,16]]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 2])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  5,  2,  6],\n",
       "        [ 9, 13, 10, 14],\n",
       "        [ 3,  7,  4,  8],\n",
       "        [11, 15, 12, 16]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatinate each of the layers next to eachother\n",
    "print(a.shape)\n",
    "b = torch.cat(torch.unbind(a),1)\n",
    "print(b.shape)\n",
    "b = torch.reshape(b,(4,4))\n",
    "print(b.shape)\n",
    "b = torch.transpose(b,0,1)\n",
    "b = torch.cat(torch.split(b,2,0),1)\n",
    "torch.transpose(torch.reshape(b,(4,4)),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a is torch.Size([1, 4, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  5,  2,  6],\n",
       "          [ 9, 13, 10, 14],\n",
       "          [ 3,  7,  4,  8],\n",
       "          [11, 15, 12, 16]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing of above kern_upscale function with 4D tensor (what the model will acually use)\n",
    "a = torch.tensor([[[[1,2],[3,4]],\n",
    "                [[5,6],[7,8]],\n",
    "                [[9,10],[11,12]],\n",
    "                [[13,14],[15,16]]]])\n",
    "print(f'shape of a is {a.shape}')\n",
    "\n",
    "b = torch.cat(torch.unbind(a,1),2)\n",
    "b = torch.reshape(b,(1,1,4,4))\n",
    "b = torch.transpose(b,2,3)\n",
    "b = torch.cat(torch.split(b,2,2),3)\n",
    "b = torch.transpose(torch.reshape(b,(1,1,4,4)),2,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a is torch.Size([2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  5,  2,  6],\n",
       "          [ 3,  7,  4,  8]]],\n",
       "\n",
       "\n",
       "        [[[ 2, 10,  4, 12],\n",
       "          [ 6, 14,  8, 16]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upscale along just one axis\n",
    "a = torch.tensor([[[[1,2],[3,4]],\n",
    "                [[5,6],[7,8]]],[[[2,4],[6,8]],[[10,12],[14,16]]]])\n",
    "print(f'shape of a is {a.shape}')\n",
    "\n",
    "# Version for doubling height\n",
    "# b = torch.cat(torch.unbind(a,2),2)\n",
    "# b = torch.reshape(b,(1,1,2,4))\n",
    "# b = torch.cat(torch.split(b,2,3),2)\n",
    "\n",
    "## b = torch.unsqueeze(torch.cat(torch.unbind(a,2),1),0)\n",
    "\n",
    "# Version for doubling width\n",
    "b = torch.cat(torch.unbind(a,3),1)\n",
    "torch.unsqueeze(torch.transpose(b, 1,2),1)\n",
    "\n",
    "#b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1 = CNNIL(axs = 'hw')\n",
    "net_2 = CNNIL(axs = 'w')\n",
    "\n",
    "# \"... trained the CNN for 40 epochs, starting with a learning rate of 0.001 and decreasing\n",
    "# the learning rate to 0.0001 after the first 20 epochs\"\n",
    "optimizer_1 = optim.Adam(net_1.parameters(), lr=0.001)\n",
    "optimizer_2 = optim.Adam(net_2.parameters(), lr=0.001)\n",
    "\n",
    "# They have a custom loss function that incorporates the final results and the result\n",
    "# right after the upscaling step\n",
    "# https://discuss.pytorch.org/t/custom-loss-functions/29387\n",
    "\n",
    "def intermediate_loss(output_intermediate, output_final, target):\n",
    "    mae_loss = nn.L1Loss() #Built in mean absolute error loss function\n",
    "    loss = mae_loss(output_intermediate, target)+mae_loss(output_final, target)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "(193, 256, 256)\n",
      "[2, 2, 1]\n",
      "shape of image = (192, 256, 256)\n",
      "patch size = [14, 14, 1]\n",
      "step size = [10, 10, 2]\n",
      "patch guess = 57600\n",
      "stack size = (57600, 14, 14, 1)\n",
      "57599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14616it [00:13, 1080.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 14616\n",
      "Number of blank patches: 42984\n",
      "shape of image = (96, 128, 256)\n",
      "keeping blank\n",
      "patch size = [7, 7, 1]\n",
      "step size = [5, 5, 2]\n",
      "patch guess = 57600\n",
      "stack size = (57600, 7, 7, 1)\n",
      "57599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14616it [00:13, 1066.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sr_train_1 = SrGen('../data/CNNIL_nifti/Raw/','../data/CNNIL_nifti/HR_train_1/','../data/CNNIL_nifti/LR_train_1/')\n",
    "\n",
    "temp = sr_train_1.get_template()\n",
    "temp['out_type'] = 'nii'\n",
    "temp['resolution'] = [2,2,1]\n",
    "temp['translation'] = [0, 0, 0]\n",
    "temp['rotation'] = [0, 0, 0]\n",
    "temp['keep_blank'] = False\n",
    "temp['same_size'] = False\n",
    "temp['patch'] = [14,14,1] #[x,y,z] when looking at the brain from the top down\n",
    "temp['step'] = [10,10,2]\n",
    "sr_train_1.set_template(temp)\n",
    "\n",
    "sr_train_1.run(clear=True, save=True)\n",
    "# sr_train_1.match_altered(update=True, paths=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "(193, 256, 256)\n",
      "[1, 1, 2]\n",
      "shape of image = (193, 256, 256)\n",
      "patch size = [1, 7, 14]\n",
      "step size = [2, 20, 20]\n",
      "patch guess = 16393\n",
      "stack size = (16393, 1, 7, 14)\n",
      "16392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3728it [00:03, 1080.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 3728\n",
      "Number of blank patches: 12665\n",
      "shape of image = (193, 256, 128)\n",
      "keeping blank\n",
      "patch size = [1, 7, 7]\n",
      "step size = [2, 20, 10]\n",
      "patch guess = 16393\n",
      "stack size = (16393, 1, 7, 7)\n",
      "16392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3728it [00:03, 979.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sr_train_2 = SrGen('../data/CNNIL_nifti/Raw/','../data/CNNIL_nifti/HR_train_2/','../data/CNNIL_nifti/LR_train_2/')\n",
    "temp = sr_train_2.get_template()\n",
    "temp['out_type'] = 'nii'\n",
    "temp['resolution'] = [1,1,2]\n",
    "temp['translation'] = [0, 0, 0]\n",
    "temp['rotation'] = [0, 0, 0]\n",
    "# temp['scale']= [1,1,1]\n",
    "temp['keep_blank'] = False\n",
    "temp['same_size'] = False\n",
    "temp['patch'] = [1,7,14] #[x,y,z] when looking at the brain from the top down\n",
    "temp['step'] = [2,20,20]\n",
    "sr_train_2.set_template(temp)\n",
    "\n",
    "sr_train_2.run(clear=True, save=True)\n",
    "#sr_train_2.match_altered(update=True, paths=False, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sr_class, axs='hw'):\n",
    "        self.sr_class = sr_class\n",
    "        self.axs = axs\n",
    "\n",
    "        # In case I forget to run match_altered before pulling the class\n",
    "        if not sr_class.HR_files:\n",
    "            sr_class.match_altered(update=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sr_class.HR_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        Y, X = self.sr_class.load_image_pair(index)\n",
    "\n",
    "        # SR along two axes\n",
    "        if self.axs == 'hw':\n",
    "            X = torch.unsqueeze(torch.squeeze(torch.tensor(X, dtype=torch.float32),-1),0)\n",
    "            Y = torch.unsqueeze(torch.squeeze(torch.tensor(Y, dtype=torch.float32),-1),0)\n",
    "\n",
    "        # SR along one axis\n",
    "        elif self.axs == 'h':\n",
    "            X = torch.squeeze(torch.tensor(X, dtype=torch.float32),-1)\n",
    "            Y = torch.squeeze(torch.tensor(Y, dtype=torch.float32),-1)\n",
    "        elif self.axs == 'w':\n",
    "            raise NotImplementedError('Super resolution along width not currently supported')\n",
    "        else:\n",
    "            raise NotImplementedError(f'Super resolution along {self.axs} currently not supported')\n",
    "            \n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 128,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 3}\n",
    "\n",
    "training_set_1 = Dataset(sr_train_1, axs = 'hw')\n",
    "training_generator_1 = torch.utils.data.DataLoader(training_set_1, **params)\n",
    "\n",
    "\n",
    "training_set_2 = Dataset(sr_train_2, axs = 'h')\n",
    "training_generator_2 = torch.utils.data.DataLoader(training_set_2, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 0, loss mean: 39.683535882701044, loss: 25.732463836669922-125.50652313232422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:18<04:21, 18.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 1, loss mean: 21.140986085974653, loss: 16.00338363647461-27.926010131835938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:38<04:09, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 2, loss mean: 15.241460219673488, loss: 11.922856330871582-18.2259578704834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:57<03:51, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 3, loss mean: 12.714632084058678, loss: 10.790395736694336-14.660911560058594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [01:17<03:33, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 4, loss mean: 11.583752117986265, loss: 9.758451461791992-14.127046585083008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [01:40<03:27, 20.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 5, loss mean: 10.670327858302905, loss: 9.081177711486816-12.263397216796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [01:59<03:03, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 6, loss mean: 10.074368352475373, loss: 8.855419158935547-11.703458786010742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [02:20<02:43, 20.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 7, loss mean: 9.610449144114618, loss: 8.563833236694336-11.503731727600098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [02:40<02:22, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 8, loss mean: 9.09516892640487, loss: 8.153694152832031-10.472452163696289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [03:00<02:01, 20.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 9, loss mean: 8.946892754927925, loss: 7.788918495178223-10.219636917114258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [03:20<01:40, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 10, loss mean: 8.557588212386422, loss: 7.583730220794678-9.512673377990723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [03:42<01:22, 20.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 11, loss mean: 8.416303149513576, loss: 7.361637115478516-9.685577392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [04:04<01:02, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 12, loss mean: 8.456875237174657, loss: 7.399376392364502-11.131046295166016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [04:24<00:41, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 13, loss mean: 8.305838945637579, loss: 7.235193252563477-9.894247055053711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [04:47<00:21, 21.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_1 : epoch 14, loss mean: 7.883763238658076, loss: 7.069115161895752-8.801149368286133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:10<00:00, 20.67s/it]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 0, loss mean: 55.53704388936361, loss: 35.535560607910156-112.98587799072266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:12<02:51, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 1, loss mean: 24.76580721537272, loss: 17.449665069580078-35.51130676269531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:24<02:38, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 2, loss mean: 13.96442855199178, loss: 11.258182525634766-17.751529693603516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:36<02:26, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 3, loss mean: 10.330316416422527, loss: 8.516886711120605-11.893511772155762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:48<02:13, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 4, loss mean: 8.475923347473145, loss: 7.364401817321777-10.016525268554688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [01:00<02:01, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 5, loss mean: 7.510581827163696, loss: 6.540799140930176-8.315034866333008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [01:12<01:49, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 6, loss mean: 6.978345966339111, loss: 6.124776840209961-7.899375915527344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [01:24<01:36, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 7, loss mean: 6.498832273483276, loss: 5.253299713134766-7.203611373901367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [01:37<01:24, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 8, loss mean: 6.447013266881307, loss: 5.573140621185303-7.4939985275268555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [01:50<01:14, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 9, loss mean: 6.101349290211996, loss: 5.331014633178711-6.848033905029297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [02:03<01:02, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 10, loss mean: 5.9397130330403645, loss: 5.364993095397949-6.554086685180664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [02:15<00:50, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 11, loss mean: 5.838910881678263, loss: 4.960616111755371-6.642794609069824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [02:28<00:37, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 12, loss mean: 6.006017080942789, loss: 5.472186088562012-6.903541088104248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [02:41<00:25, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 13, loss mean: 6.923411083221436, loss: 4.3185858726501465-9.001203536987305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [02:55<00:13, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNIL_2 : epoch 14, loss mean: 5.9070786794026695, loss: 5.139086723327637-7.809691429138184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:09<00:00, 12.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "max_epochs = 15\n",
    "save_rate = 10\n",
    "epoch_adjust = 0\n",
    "save_prefix = \"./CNNIL_save_\"\n",
    "\n",
    "mean_loss = {'CNNIL_1': [],\n",
    "            'CNNIL_2':[]}\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    losses = []\n",
    "\n",
    "    # Training\n",
    "    count = 0\n",
    "    for inp, goal in training_generator_1:\n",
    "        optimizer_1.zero_grad()\n",
    "\n",
    "        output_1, output_2 = net_1(inp) # the 2 is the number of iterations in the LISTA network\n",
    "        #output = torch.clamp(output, 0, 255)\n",
    "\n",
    "        loss = intermediate_loss(output_1,output_2,goal)\n",
    "        loss.backward()\n",
    "        optimizer_1.step()\n",
    "        #print(f'loss = {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "        count = count+1\n",
    "    \n",
    "\n",
    "    # if (epoch % save_rate == 0) or epoch == (max_epochs-1):\n",
    "    #     torch.save(net.state_dict(), f'{save_prefix}{epoch+epoch_adjust}.p')\n",
    "    print(f'CNNIL_1 : epoch {epoch}, loss mean: {sum(losses)/len(losses)}, loss: {min(losses)}-{max(losses)}')\n",
    "    mean_loss[\"CNNIL_1\"].append(sum(losses)/len(losses))\n",
    "\n",
    "    # Give computer time to cool down\n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    losses = []\n",
    "\n",
    "    # Training\n",
    "    count = 0\n",
    "    for inp, goal in training_generator_2:\n",
    "        optimizer_2.zero_grad()\n",
    "\n",
    "        output_1, output_2 = net_2(inp) # the 2 is the number of iterations in the LISTA network\n",
    "        #output = torch.clamp(output, 0, 255)\n",
    "\n",
    "        loss = intermediate_loss(output_1,output_2,goal)\n",
    "        loss.backward()\n",
    "        optimizer_2.step()\n",
    "        #print(f'loss = {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "        count = count+1\n",
    "    \n",
    "\n",
    "    # if (epoch % save_rate == 0) or epoch == (max_epochs-1):\n",
    "    #     torch.save(net.state_dict(), f'{save_prefix}{epoch+epoch_adjust}.p')\n",
    "    print(f'CNNIL_2 : epoch {epoch}, loss mean: {sum(losses)/len(losses)}, loss: {min(losses)}-{max(losses)}')\n",
    "    mean_loss[\"CNNIL_2\"].append(sum(losses)/len(losses))\n",
    "\n",
    "    # Give computer time to cool down\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7a1c3cdfa0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkr0lEQVR4nO3deXTc5X3v8fd3Ni0jydJI8ibZlsEGgk1sbLEVKFtIgFCWhEJyc1p6mxyae5sLlNCGND03zS1p6UIT2ua2zUJxT5uYJVAoDTQE8GUJMcjYBoMBL3hftMuytWue+8f8ZCuuZW0zmt/vp8/rHJ1ZNNJ85Tx88vyeeRZzziEiIsETyXcBIiIyMQpwEZGAUoCLiASUAlxEJKAU4CIiARWbyjerqqpydXV1U/mWMo2sW7eu2TlXnY/3VtuWXBqpbU9pgNfV1dHQ0DCVbynTiJntzNd7q21LLo3UtjWEIiISUApwEZGAUoCLiATUmAPczKJmtt7MnvYeLzSztWa21cweNrNE7soUEZHjjacHfgewedjjPwe+5ZxbBLQBn89mYSIicnJjCnAzqwU+CXzfe2zA5cBj3ktWATfkoD4RERnBWHvg3wb+AEh7jyuBdufcgPd4D1Bzoh80s9vMrMHMGpqamiZTq4iIDDNqgJvZtUCjc27dRN7AOfdd51y9c66+uvrEayy+99J2nt10YCK/XsS3Pmw+wv0/fZ/9Hd35LkVCaiw98AuB68xsB7CazNDJA0C5mQ0tBKoF9k60iH/+xQ6e3bR/oj8u4ksHOnr42xe28mHTkXyXIiE1aoA7577qnKt1ztUBnwFecM59DngRuMl72a3AkxMtojJZQMuRvon+uIgvpZKZiVmtXWrbkhuTmQf+FeAuM9tKZkz8BxP9RVUlCZoPq5FLuBwNcHVOJEfGtReKc24NsMa7vx04NxtFVCYLeGtPRzZ+lYhvVBTHAWhR50RyxBcrMStLErQe6SOd1vmcEh6xaIQZRXHaNIQiOeKTAC9gIO041NOf71JEsqoymdDnO5IzvgjwqpLMWKHGwSVsUskErWrXkiO+CPDKZAEALYd781yJSHZVJBMaQpGc8UeAez1wXWpK2GgIRXJJAS6SQ6lkgrYjfTinD+gl+3wR4KliL8A1hCIhk0omvA/oB0Z/scg4+SLAY9EIFcVxzZeV0NFiHsklXwQ4ZKYSthxRD1zC5ViAq21L9vknwJNaTi/hcyzAtcZBss83AV5VUqAxcAkd9cAll3wT4ClNt5IQOrrGQW1bcsA3AV5ZkqC9q5/+wfToLxYJiKJElKJ4VKsxJSd8FOCZnkqbeioSMqlkQnuCS074JsCrktoPRcIplUxoGqHkxLj2A8+loR64phLKVPKOCuwEBoEB51y9maWAh4E6YAdws3OubaLvoQCXXPFND/zocnr1wGXqXeacW+6cq/ce3wM875xbDDzvPZ4wBbjkim8CvMr7tL5ZUwkl/64HVnn3VwE3TOaXKcAlV3wT4GVFMWIR03QrmWoO+KmZrTOz27znZjnn9nv3DwCzTvSDZnabmTWYWUNTU9OIb5BKJujqG6SnfzCrhYv4JsDNjMqShBbzyFS7yDm3Arga+F0z+9Xh33SZbQRPuJWgc+67zrl651x9dXX1iG+g/VAkV3wT4JBZ9KAxcJlKzrm93m0j8ASZg7oPmtkcAO+2cTLvoQCXXPFXgJckaFYjlyliZkkzKx26D3wc2AQ8BdzqvexW4MnJvE9lUvvdS26MOo3QzAqBl4AC7/WPOee+bmYPAZcAHd5Lf8s5t2EyxVSVFPBh85HJ/AqR8ZgFPGFmkGnbP3TOPWtmbwCPmNnngZ3AzZN5kwovwLVITbJtLPPAe4HLnXOHzSwOvGJmz3jf+33n3GPZKqYymdAQikwZ59x2YNkJnm8BrsjW+6gHLrky6hCKyzjsPYx7Xzk5H6qypIDu/kG6+nR6iYRHWWGcaMS0I6Fk3ZjGwM0samYbyHyY85xzbq33rW+a2Vtm9i0zKxjhZ8c01Qq0mEfCKRIxKorj2hNcsm5MAe6cG3TOLQdqgXPNbCnwVeAM4BwgBXxlhJ8d01QrgCodbiwhlVnMox64ZNe4ZqE459qBF4GrnHP7veGVXuCfyEy/mpSjeydrLriEjFZjSi6MGuBmVm1m5d79IuBK4L1h82SNzFLjTZMtRkMoElYKcMmFscxCmQOsMrMomcB/xDn3tJm9YGbVgAEbgC9OtpihHnizLjUlZBTgkgujBrhz7i3g7BM8f3m2iylKREkmouqBS+ikkgW0d/czmHZEI5bvciQkfLUSEzJTCTUGLmGTKo7jHLTrZB7JIh8GuA43lvBJeQeWaBhFssl/AZ5M6Fg1CR2txpRc8GGAawhFwqeiWPuhSPb5L8BLMp/Wp9M5Wa0vkheVWqQmOeDDAC9gIO041KNlxxIeQz1wjYFLNvkuwIeW02scXMIkEYtQWhBTgEtW+S7AtZxewipVosU8kl3+C3CNFUpIaTWmZJt/A1w9cAmZVLECXLLLdwGeKtYYuISTeuCSbb4L8Fg0QkVxnBZtaCUhMzQG7pymyEp2+C7AYWg/FPVUJFxSxQn6BtMc6RvMdykSEv4McB1uLCGU8pbTt6ptS5b4MsCrSgq0J7iEzrEZVmrbkh2+DPBKzZeVEDq6H4q2lJUs8WeAJwto7+qnfzCd71JEsubYIjUFuGSHPwO8RDu3SfikSrQfimSXLwNc+6FIGCUTURKxCK0aQpEs8WWAV3qnl+jDHgkTM8usxlTHRLLEnwE+dHqJGrqEjFZjSjb5M8C9Hniz9kORkKksSWgIRbJm1AA3s0Ize93MNprZO2b2De/5hWa21sy2mtnDZpbIVlFlhTHiUdOOhBI6FdrQSrJoLD3wXuBy59wyYDlwlZmdD/w58C3n3CKgDfh8tooyM52NKaGUSmoMXLJn1AB3GYe9h3HvywGXA495z68CbshmYSktp5cpYmZRM1tvZk97j3N2dVmZTNDZO0DfgNY4yOSNaQzca+AbgEbgOWAb0O6cG/BesgeoGeFnbzOzBjNraGpqGnNhlSUJmnWpKVPjDmDzsMc5u7qsSGo1pmTPmALcOTfonFsO1ALnAmeM9Q2cc991ztU75+qrq6vHXFhViYZQJPfMrBb4JPB977GRw6tLzbCSbBrXLBTnXDvwInABUG5mMe9btcDebBamHQllinwb+ANgaEyjkhxeXabUA5csGssslGozK/fuFwFXkrncfBG4yXvZrcCT2SyssqSA7v5BuvoGRn+xyASY2bVAo3Nu3UR+fiJXl0MBrhlWkg2x0V/CHGCVmUXJBP4jzrmnzexdYLWZ3QusB36QzcKOnY3ZR3FqLGWKjNuFwHVmdg1QCJQBD+BdXXq98KxeXR7bE1zDgzJ5oyajc+4t4OwTPL+dzHh4ThzbD6WXeaniXL2NTGPOua8CXwUws0uBu51znzOzR8lcXa4my1eX5cUJzKC1qz9bv1KmMV+uxARtvSl59RXgLjPbSmZMPGtXl9GIUV4Up1X7/EgW+HZsQqeXyFRyzq0B1nj3c3p1qf1QJFt83wPXlrISNpXJAgW4ZIVvA7woESWZiGoIRUKnIhlXgEtW+DbAITOVUGOFEjYp9cAlS3we4AnNl5XQqUwmaOvqJ512+S5FAs7fAZ4s0Bi4hE5FMsFg2nGoR1MJZXJ8HeBVJQnthyKhU6nVmJIlvg7wypLMdCtdakqYHN0PRQEuk+TvAE8WMKBLTQkZ7Yci2eLvAD+6nF4NXcLj6H4oCnCZJF8HeFXJ0HJ6jYNLeCjAJVt8HeDHltOroUt4FMajFCeiCnCZNH8HeFI9cAkn7Yci2eDrAK8ojmOmMXAJn0oFuGSBrwM8Fo1QXhTXjoQSOhUKcMkCXwc4ZPZD0YZWEjYaQpFs8H+A63BjCSENoUg2+D7Aq0oKaNYQioRMRTJBd/8g3X2D+S5FAsz3AV5Zoh64hM+x/VDUOZGJ83+AJwvo6O6nbyCd71JEsiblTZFtO6JtImTi/B/g3mKeti71wiU8Usk4oB64TM6oAW5m88zsRTN718zeMbM7vOf/2Mz2mtkG7+uaXBRYdXQ/FDV0CY+hHrg+yJTJGMup9APAl51zb5pZKbDOzJ7zvvct59xf5a68zDRCQOPgEiraD0WyYdQAd87tB/Z79zvNbDNQk+vChlSqoUsIlRXGiEVM7VomZVxj4GZWB5wNrPWe+pKZvWVmD5pZRbaLg2M9cA2hSJiYmVZjyqSNOcDNrAT4MXCnc+4Q8PfAqcByMj30+0f4udvMrMHMGpqamsZdYFlhjHjUtCOhhI4W88hkjSnAzSxOJrz/1Tn3OIBz7qBzbtA5lwa+B5x7op91zn3XOVfvnKuvrq4ed4FmRmWyQDsSSuhoOb1M1lhmoRjwA2Czc+6vhz0/Z9jLbgQ2Zb+8DC3mkTDSEIpM1lhmoVwI/Abwtplt8J77Q+CzZrYccMAO4HdyUB+QGQdvVkOXkKlMJmjV+gaZhLHMQnkFsBN86yfZL+fEqpIJtjcdnqq3E5kSqWSC9q5+BgbTxKK+X1MnPhSIVqMhFAmjobngbV1aTi8TE5AAL6C7f5CuvoF8lyKSNccCXJ0TmZhgBPjQzm3qhUsWmVmhmb1uZhu9bSK+4T2/0MzWmtlWM3vYzBK5eP+U2rVMUiACvEqLeSQ3eoHLnXPLyKxnuMrMzgf+nMw2EYuANuDzuXhzLaeXyQpEgA/tSKieimSTyxj6dDzufTngcuAx7/lVwA25eP+jAa4hFJmggAS4t6GVtt6ULDOzqDc9thF4DtgGtDvnhj5w2cMIe/9MdpVxRbEX4OqYyAQFI8CTQ1vKqqFLdnmriZcDtWRWE58xjp+d1CrjeDRCWWGMVnVMZIICEeCF8SjJRFRDKJIzzrl24EXgAqDczIbWSNQCe3P1vpUlBbRqGqFMUCACHDINXUMokk1mVm1m5d79IuBKYDOZIL/Je9mtwJO5qiGzH4ratUzMWJbS+4IW80gOzAFWmVmUTGfmEefc02b2LrDazO4F1pPZCygnKooT7GnrytWvl5ALToAnC9TQJaucc2+R2d/++Oe3M8LumtlWmUzw9t72qXgrCaHADKFUlSS0J7iETqoksyOhcy7fpUgABSbAK72Gnk6roUt4pIoT9A86Onu1TYSMX3ACPFnAYNrR0a1P7CU8ju6HoqtLmYDgBPjQakw1dAmRlNq1TEJgAnxoPxQdrSZhktJqTJmEwAS4euASRtoPRSYjOAGeVA9cwmeoY6IdCWUiAhPgFcVxYhFjT1t3vksRyZqieJSCWEQBLhMSmACPRSMsn1fOz7e15LsUkawxs8zhxgpwmYDABDjARYur2LSvQ1OuJFSGFvOIjFegAvzixVU4h3rhEioVxVplLBMzaoCb2Twze9HM3vXODbzDez5lZs+Z2RbvtiLXxS6rLae0IMYrW8e/eb6IX1UmE7qqlAkZSw98APiyc+5M4Hzgd83sTOAe4Hnn3GLgee9xTsWiEc4/tZKXtzRr7wgJjVSyQEMoMiGjBrhzbr9z7k3vfieZ/ZJrgOvJnBcIOTw38HgXL65iT1s3O1u0M6GEQyoZ53DvAL0Dg/kuRQJmXGPgZlZHZvvNtcAs59x+71sHgFnZLe3ELlpUBcDLW5un4u1Ecm7ozNfGQ1rjIOMz5gA3sxLgx8CdzrlDw7/nMuMZJxzTmOzBr8dbWJWkpryIV7ZoHFzC4cw5ZQBs3NOe30IkcMYU4GYWJxPe/+qce9x7+qCZzfG+P4fMqd7/xWQPfj1BLVy0qIqfb2thYDA96d8nkm9nzi2jKB6lYUdbvkuRgBnLLBQjc6TUZufcXw/71lNkzguEHJ8beLyLFlfR2TPAW3s7puotRXImHo2wbN4M3tylAJfxGUsP/ELgN4DLzWyD93UNcB9wpZltAT7mPZ4SFy6qwgxe2aJxcAmH+gUp3tl3iK4+HewgYzeWWSivOOfMOfdR59xy7+snzrkW59wVzrnFzrmPOedap6JgyOzgtmRumQJcQmNlXQWDaceG3e35LkUCJFArMYe7aFE1b+5q47COopIQWDEvsw5uncbBZRwCG+AXL65iIO1Yu13L6iX4ZhTHOW1WCQ07FeAydoEN8JULKiiIRXhZwygSEisXpHhzV5sO7pYxC2yAF8ajnLswxSta0CMhUb+ggs6eAT5o7Mx3KRIQgQ1wyAyjbG08zP4OHfIgwVdflxkH13xwGatAB/hFizILgzQbRcJgfqqYqpIC1mkcXMYo0AF+xuxSqkoSGkaRUDAz6hdUKMBlzAId4JGIceGiKl7d2qwPfiQU6usq2NXaRWNnT75LkQAIdIBDZnfC5sN9vHdAH/xI8K1coPngMnaBD/CLF3vj4DqlR0JgydwZFMQimg8uYxL4AJ89o5BFM0s0H1zGzU/HBQ5JxCIsqy1XgMuYBD7AITOM8vqHrfT060QTGRffHBc43Mq6Ct7Z20F3n9qznFwoAvzixVX0DqT16b2Mi9+OCxxSv6CCgbTTAQ8yqlAE+HmnVBKLmIZRZML8cFzgkKMfZKpDIqMIRYCXFMRYMb+CVzUfXCbAL8cFDikvTrBoZgkNO6Zsh2YJqFAEOGRO6dm0r4O2I335LkUCxE/HBQ43tKBH6xvkZEIV4M7Bq9vUC5ex8eNxgUNWLqjgUM8A25oOT/VbS4CEJsA/WjOD0sKY9kWR8fDdcYFD6utSAJpOKCcVy3cB2RKLRviVUyt5eUszzjkynSuRkTnnXgFGaihXTGUtx6urLKYymaBhRxufPXd+PksRHwtNDxzgosXV7G3vZkdLV75LEZkUM2PFggrW7dQHmTKyUAX4xYuqAHhli5bVS/DVL6hgR0sXTZ29+S5FfCpUAb6gspjaiiLNB5dQGDrgQfPBZSShCnAz4+LFVby2rYWBwXS+yxGZlKU1M0jEIhpGkRGNGuBm9qCZNZrZpmHP/bGZ7T3uk3tfuGhRNZ29A1qGLIFXEIvy0ZoZmokiIxpLD/wh4KoTPP8t59xy7+sn2S1r4i5aVEUyEeWB57eSWUQnElwr6yrYtLdDG7XJCY0a4M65l4DAXMPNKI7z+584nZc+aOKpjfvyXY7IpNQvSNE/6Hh7b0e+SxEfmswY+JfM7C1viGXE/ZJztV/EyfzGBXUsm1fOnzz9Lu1dWlovwTW0sZVOqpcTmWiA/z1wKrAc2A/cP9ILc7lfxEiiEePPbjyLtq5+7nvmvSl5T5FcSCUTnFKd1AeZckITCnDn3EHn3KBzLg18Dzg3u2VN3plzy/jCRQtZ/cZuXv9QjV+Ca+X8zMZW+kxHjjehAB/aqc1zI7BppNfm0x0fW0xtRRFfffwtegf0IZAEU31dBW1d/WxrOpLvUsRnxjKN8EfAa8DpZrbHzD4P/IWZvW1mbwGXAb+X4zonpDgR494blrKt6Qj/sGZ7vssRmZCVCzIbW2kYRY436mZWzrnPnuDpH+Sglpy49PSZ/NqyuXznxa1cu2wOp1aX5LskkXE5tTpJRXGchh1t3HKONraSY0K1EnMk//vaMymMR/jaE29rHFECx8xY6R3wIDLctAjw6tICvnrNR/jF9lYeXbcn3+WIjNvKBSm2Nx+h5bA2tpJjpkWAA9xSP49z6ir45n9spln/EUjADG1s9eau9vwWIr4ybQI8EjH+7FNn0dU3wL1Pv5vvckTG5ayaGSSiERr0QaYMM20CHGDRzFL+xyWn8m8b9vGy9gyXACmMR1laU8Y6rciUYaZVgAP8z8sWcUpVkq89sYnuPs0Nl+A4py7Fxj3t7GzRfHDJmHYBXhiPcu+NS9nV2sXfvLAl3+WIjNlvXVhHYTzKlx/ZyGBas6lkGgY4wK+cWsWvr6zley9t5+m3tGOhBMOcGUV847olNOxs4/sva2GaTNMAB/jaJz/C0poZfOmH67n70Y0c7h3Id0kio7rx7Bo+sWQW9//0A94/0JnvciTPpm2AlxcnePSLF3D75Yt4/M09XPPAy6zfpQ+IxN/MjD+98SxKC2Pc9cgG+gZ0dOB0Nm0DHCAejXDXx0/n4d+5gMG046Z/eI2/fX6LxhfF1ypLCvjTT53FO/sO8Xf6HGdam9YBPuScuhTP3Hkx1350Dvc/9wGf+e5r7G7tyndZIiP6xJLZfGpFDd9Zs42Nu9vzXY7kiQLcU1YY54HPnM23b1nOe/s7ueaBl3lyw958lyUyoq//2hJmlhZw1yMbdGbmNKUAP84NZ9fwkzsu5vTZpdyxegN3rl7PoZ7+fJcl8l/MKIrzlzctY1vTEf7i2ffzXY7kgQL8BOalill92/ncdeVp/Ptb+7n62y/z7KYD2slQfOeixVX85gULePDVD3ltW0u+y5EppgAfQSwa4fYrFvPoFy8gWRDli/+yjv/2vbVs3n8o36WJ/JJ7rj6Duspi7n50I526WpxWFOCjWDG/gp/cfjH/5/olbD5wiE/+zcv84RNva1tP8Y3iRIz7b17O/o5u7n16c77LkSmkAB+DWDTCb15Qx5q7L+U3L6jj4Td2c+lfreH7L2/XPFzxhZULKvidS07l4YbdvPDewXyXI1NEAT4O5cUJ/vi6JfznnRdz9vwK7v2PzVz17Zd44b2DGh8PIDN70MwazWzTsOdSZvacmW3xbivyWeN43PmxxZwxu5Sv/Pht2o705bscmQIK8AlYNLOUVf/9HB78rXoAfvuhBm79pzfYclBLmwPmIeCq4567B3jeObcYeN57HAgFsSh/ffNy2rv6uPvRjdptcxpQgE+QmXH5GbN49s5f5Y8++RHW72rjym+9xC3/+BqPNuzmiPZW8T3n3EvA8SckXA+s8u6vAm6Yypom68y5ZfzRJ8/khfcbueE7r7K1UZ2KMFOAT1IiFuELF5/Cmrsv5e6Pn0ZjZy+//9hbnPPNn/HlRzby2rYW0lqaHySznHP7vfsHgFkjvdDMbjOzBjNraGryzwEht/5KHf/82+fSfLiXX/vbV3lM58CGlo02dmtmDwLXAo3OuaXecyngYaAO2AHc7JwbdSeo+vp619DQMMmS/c05x5u72nhs3R7+feN+DvcOUFtRxKdX1PLpFbXMryzOd4mhZWbrnHP14/yZOuDpYW273TlXPuz7bc65UcfB/di2Gw/1cPvq9fxieyufXlHLn9ywhOJELN9lyQSM1LbH0gN/iBCNE+aambFyQYo/+9RHeeNrH+OBzyxnYVWSv3lhC7/6ly9yyz++xurXd9F4qCffpcqJHTSzOQDebWOe65mwmWWF/OsXzueOKxbz+Po9XPd3r2oL2pAZ9f+OnXMveb2U4a4HLvXurwLWAF/JZmFhUJSIcv3yGq5fXsO+9m6eWL+Xx9bt4Z7H3wZgaU0Zl50+k8vOmMmy2nKiEctzxQI8BdwK3OfdPpnfciYnGjF+78rTOG9hittXb+D677zCN65bws318zBTewu6UYdQ4OSXmZZpBW3DLzuP+9nbgNsA5s+fv3Lnzp1ZKTyonHO8d6CTF95rZM37jazb2UbaQSqZ4JLTqrnsjJlcsriaGcXxfJcaOOMdQjGzH5HpiFQBB4GvA/8GPALMB3aSGR4c9Sh4Pw6hHK+xs4ffe3gDr25t4Yblc/nmjWeRLNCQShCM1LYnHeDe48COE+Zbe1cf/++DJta838Sa9xtp6+onYpmFGVd8ZBZXL53NgspkvssMhImMgWdLUNr2YNrxnRe38u2ffUBdZZL7b17G2fMDM9V92hqpbU/0/34Pmtkc59z+oI8T5lt5ceLoMMtg2rFhdztr3m/khfcaue+Z97jvmff4yJwyrl46m6uXzmbxrNJ8lywBFo0Yt1+xmHMXprj9R+u58f/+nKU1ZdxSP4/rltcwo0hXfkEy0R74XwItzrn7zOweIOWc+4PRfk9Qeil+saeti2c3HeDZTQdYt6sN5+DU6iRXL53DVUtns2RumcYxh1EPfHw6uvt5csNeVr++m3f3H6IgFuGas+ZwyznzOG9hSm3LRyY8hDLdxgn9qvFQD//5zgGe2XSAX2xvIe1gfqqYq5bOZsX8CqpLC5hZWkB1aQGF8Wi+y80LBfjEbdrbweo3dvHkhn109gywsCrJr9fXctOKWmaWFea7vGlvUmPg2RL0Ru4XLYd7+dnmgzyz6QCvbm2mf/CX/zcsLYhRXVpAlRfo1SWZ29NnlXLOwlRoL5MV4JPX3TfIM5v28/Abu1n7YSvRiHHZ6TP5xJJZnD2/nFOqSohottSUU4CHVGdPPztbumg63EtT57Av73Gzd9vZk1nabwYfmV3GeaekOG9hinMXVpJKJvL8V2SHAjy7Pmw+wiMNu3ls3R6aOjPbJ5cWxlhWW87yeZmvZfPKqS4tyHOl4acAn+a6+gbYuLuDtR+2sHZ7K2/uaqPX2wr3tFklnLswxXkLKzlvYSqwl8wK8NxIpx3bmg6zfnc7G3a3s2FXO+8f7GTQ2yKipryI5fPLOXteOWfVzODMuWWUFobzKm+8jvQOEI3YpIc1FeDyS3oHBnl7TwdrP2xl7YetrNvRyhFv97qieJSq0gRVJQVUJguo9u5XlRRQWTL0fIK0g/7BNL0DafoH0/QNu+3zbp2DimTi6Ph8VUkBiVhutuBRgE+d7r5BNu3rYMMuL9R3t7O3vfvo9xdUFrN0bibMl9bMYMncMqpKpkdPvX8wzUsfNPH4m3t5bvNBCqIRPrWihs+dv4DTJjiLTAEuJzUwmGbTvkM07Ghlf0cPLYd7aT7cR7N323qkl2ztyVVRHPc+dC3MjNGXFlBTXsR5p6Q4fVbphGc/KMDzq7Gzh3f2HuKdfR1s2nuId/Z3sLv1WKjPLitkydwyzpxbxoyiOLGIEY9FiEcixKJGPBohHjVikQjxWISCWISa8iLmzCgkFvX/vnvv7Ovgx+v28tTGvTQf7iOVTHDdsrl0dPfzH2/tp28wzbl1KT53/nyuWjqbgtjYe+UKcJmUwbSjrauPFi/UW4/0ETEjEcv8R5eIRUhEI97jyNHHAK1H+mjq7KXx6Ph8D42Hjo3TN3b2Hj3ZqKqkgAsXVXLhoiouXFRFTXnRmGtUgPtPR1c/7+zv4N19h3hn3yE27e1gW9PhcXUG4lGjtqKYBZXFLEgVs6AySV1VMfNTSealisYVhNnW2NnDk+v38eM39/DegU7iUeOKM2bx6ZW1XHJa9dGrzdYjfTzasJsfvr6LnS1dVCYT/Hr9PD533nzmpUbf4E4BLr7lnGNvezc/39bCq1ubeXVrM82HMyfKnFKVPBrmF5xSedItBhTgwdA3kKZnYJD+gTQDaUf/YJr+QcfA0G06MxTX059mT1sXO1q62NXSxY6WI+xs6eLwsL32zTI9+/LiBGWFMWYUxZlRFKfMu83czzxfWhinKB6lOBGlKBGlKJ65TUQjJ7zqS6cdnb0DHOrup8P7au86dn/thy289EETaQfL5pVz04oarv3oXCpOMikgnXa8srWZf/nFTn62+SAOuOS0aj533gIuP2PmiPshKcAlMJxzvH+wk1e2NPPzbS38YnsLXX2DRAzOqi3n4dvOP+GHQgrw8HPO0XKkj50tXez0An13W9fRkD3UPXA0YLv7x3YiUcQyB0MXxqMUJSIYRkd3P509/Se9Upg7o5Abzq7hUytqWTSzZNx/y/6Obn70+u7M7qSdvXx6RS3337zshK/N9lJ6kZwxM86YXcYZs8v4wsWn0DeQZuOedl7Z0szOliPTdqGSZNrG0AfqKxecfA+XvoE0h3r6j4Z7Z88A3f2DdPcNnvC2q2+Qnv5B0s4d7b0P9ebLhx4XH3u+KB6d1GrVOTOKuOvK0/hfly/i+c0HJzT7SwEuvpeIRTinLsU5dal8lyIBkohFjoa9n8WjEa5aOmdCP+v/j3ZFROSEFOAiIgGlABcRCSgFuIhIQCnARUQCSgEuIhJQCnARkYBSgIuIBNSULqU3syYyR7CdSBXQPGXF5EYY/gYI7t+xwDlXnY83PknbDuq/5fHC8HcE+W84Ydue0gA/GTNryNc+FtkShr8BwvN3+EFY/i3D8HeE4W84noZQREQCSgEuIhJQfgrw7+a7gCwIw98A4fk7/CAs/5Zh+DvC8Df8Et+MgYuIyPj4qQcuIiLjoAAXEQmovAe4mV1lZu+b2VYzuyff9UyUme0ws7fNbIOZBeJsLTN70MwazWzTsOdSZvacmW3xbk9+7ImMSG07f6ZL285rgJtZFPgOcDVwJvBZMzsznzVN0mXOueUBmmv6EHDVcc/dAzzvnFsMPO89lnFS2867h5gGbTvfPfBzga3Oue3OuT5gNXB9nmuaNpxzLwGtxz19PbDKu78KuGEqawoRte08mi5tO98BXgPsHvZ4j/dcEDngp2a2zsxuy3cxkzDLObffu38AmJXPYgJMbdt/Qte2dahx9lzknNtrZjOB58zsPa8XEFjOOWdmmmcqats+le8e+F5g3rDHtd5zgeOc2+vdNgJPkLmEDqKDZjYHwLttzHM9QaW27T+ha9v5DvA3gMVmttDMEsBngKfyXNO4mVnSzEqH7gMfBzad/Kd86yngVu/+rcCTeawlyNS2/Sd0bTuvQyjOuQEz+xLwn0AUeNA5904+a5qgWcATZgaZf9MfOueezW9JozOzHwGXAlVmtgf4OnAf8IiZfZ7M9qg356/C4FLbzq/p0ra1lF5EJKDyPYQiIiITpAAXEQkoBbiISEApwEVEAkoBLiISUApwEZGAUoCLiATU/wfzFPaaz0tzIwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].plot([x for x in range(len(mean_loss[\"CNNIL_1\"]))],mean_loss[\"CNNIL_1\"])\n",
    "axs[1].plot([x for x in range(len(mean_loss[\"CNNIL_1\"]))],mean_loss[\"CNNIL_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "(193, 256, 256)\n",
      "[2, 2, 1]\n",
      "shape of image = (192, 256, 256)\n",
      "patch size = [14, 14, 1]\n",
      "step size = [10, 10, 2]\n",
      "patch guess = 57600\n",
      "stack size = (57600, 14, 14, 1)\n",
      "57599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15052it [00:13, 1144.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 15052\n",
      "Number of blank patches: 42548\n",
      "shape of image = (96, 128, 256)\n",
      "keeping blank\n",
      "patch size = [7, 7, 1]\n",
      "step size = [5, 5, 2]\n",
      "patch guess = 57600\n",
      "stack size = (57600, 7, 7, 1)\n",
      "57599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15052it [00:17, 865.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed successfully\n",
      "Clearing existing output directories\n",
      "(193, 256, 256)\n",
      "[1, 1, 2]\n",
      "shape of image = (193, 256, 256)\n",
      "patch size = [1, 7, 14]\n",
      "step size = [2, 20, 20]\n",
      "patch guess = 16393\n",
      "stack size = (16393, 1, 7, 14)\n",
      "16392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3871it [00:05, 703.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 3871\n",
      "Number of blank patches: 12522\n",
      "shape of image = (193, 256, 128)\n",
      "keeping blank\n",
      "patch size = [1, 7, 7]\n",
      "step size = [2, 20, 10]\n",
      "patch guess = 16393\n",
      "stack size = (16393, 1, 7, 7)\n",
      "16392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3871it [00:04, 817.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate testing data for both axes\n",
    "sr_test_1 = SrGen('../data/CNNIL_nifti/Raw_test/','../data/CNNIL_nifti/HR_test_1/','../data/CNNIL_nifti/LR_test_1/')\n",
    "\n",
    "temp = sr_test_1.get_template()\n",
    "temp['out_type'] = 'nii'\n",
    "temp['resolution'] = [2,2,1]\n",
    "temp['translation'] = [0, 0, 0]\n",
    "temp['rotation'] = [0, 0, 0]\n",
    "temp['keep_blank'] = False\n",
    "temp['same_size'] = False\n",
    "temp['patch'] = [14,14,1] #[x,y,z] when looking at the brain from the top down\n",
    "temp['step'] = [10,10,2]\n",
    "sr_test_1.set_template(temp)\n",
    "\n",
    "sr_test_1.run(clear=True, save=True)\n",
    "#sr_test_1.match_altered(update=True, paths=False, sort=False)\n",
    "\n",
    "\n",
    "sr_test_2 = SrGen('../data/CNNIL_nifti/Raw_test/','../data/CNNIL_nifti/HR_test_2/','../data/CNNIL_nifti/LR_test_2/')\n",
    "temp = sr_test_2.get_template()\n",
    "temp['out_type'] = 'nii'\n",
    "temp['resolution'] = [1,1,2]\n",
    "temp['translation'] = [0, 0, 0]\n",
    "temp['rotation'] = [0, 0, 0]\n",
    "# temp['scale']= [1,1,1]\n",
    "temp['keep_blank'] = False\n",
    "temp['same_size'] = False\n",
    "temp['patch'] = [1,7,14] #[x,y,z] when looking at the brain from the top down\n",
    "temp['step'] = [2,20,20]\n",
    "sr_test_2.set_template(temp)\n",
    "\n",
    "sr_test_2.run(clear=True, save=True)\n",
    "#sr_test_2.match_altered(update=True, paths=False, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_t = {'batch_size': 1,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 3}\n",
    "\n",
    "testing_set_1 = Dataset(sr_test_1, axs = 'hw')\n",
    "testing_generator_1 = torch.utils.data.DataLoader(testing_set_1, **params_t)\n",
    "\n",
    "\n",
    "testing_set_2 = Dataset(sr_train_2, axs = 'h')\n",
    "testing_generator_2 = torch.utils.data.DataLoader(testing_set_2, **params_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run both datasets through the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR and LR file locations updated\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "# Load trained models:\n",
    "#net_1.load_state_dict(torch.load('./MRI_reflect_pad_save_39.p'))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    comp={'psnr' : [], 'rmse' : []}\n",
    "    for idx, [im_l, im_h] in enumerate(testing_generator_1):\n",
    "\n",
    "        # Take low resolution and upscale using bicubic interpolation\n",
    "        # (which has already been done due to the image generation process)\n",
    "        # Thus im_l is the bicubic interpolation to compare to...\n",
    "\n",
    "        # Use SR model on low resolution image\n",
    "        output_1, output_2 = net_1(im_l)\n",
    "\n",
    "        # Upscale im_l to the same size as im_h\n",
    "        im_l = torch.tensor(resize(im_l, im_h.shape, order=1, mode = 'symmetric'))\n",
    "\n",
    "        # Calculate PSNR for bicubic\n",
    "        # im_l = np.rint( np.clip(im_l, 0, 255))\n",
    "        # im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = im_l - im_h\n",
    "        rmse_b = np.sqrt((diff**2).mean())\n",
    "        psnr_b = 20*np.log10(im_h.max()/rmse_b)\n",
    "\n",
    "        #print(f'bicubic evaluation for {idx}: rms={rmse_b}, psnr={psnr_b}')\n",
    "\n",
    "        # Calculate PSNR for SR\n",
    "        # im_h_sr = np.rint( np.clip(im_h_sr, 0, 255))\n",
    "        # im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = output_2 - im_h\n",
    "        rmse_s = np.sqrt((diff**2).mean())\n",
    "        psnr_s = 20*np.log10(im_h.max()/rmse_s)\n",
    "        #print(f'SR evaluation for {idx}: rms={rmse_s}, psnr={psnr_s}')\n",
    "        comp['psnr'].append(psnr_s-psnr_b)\n",
    "        comp['rmse'].append(rmse_s-rmse_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'RMSE: SR - BiC')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check, plot a histogram of how well the SR image performed compared to the Bicubic comparison\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].hist([float(x) for x in comp['psnr'][0:-1:300]], bins=10)\n",
    "axs[0].set_title('PSNR: SR - BiC')\n",
    "axs[1].hist([float(x) for x in comp['rmse'][0:-1:300]], bins=10)\n",
    "axs[1].set_title('RMSE: SR - BiC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models:\n",
    "#net_1.load_state_dict(torch.load('./MRI_reflect_pad_save_39.p'))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    comp={'psnr' : [], 'rmse' : []}\n",
    "    for idx, [im_l, im_h] in enumerate(testing_generator_2):\n",
    "\n",
    "        # Use SR model on low resolution image\n",
    "        output_1, output_2 = net_2(im_l)\n",
    "\n",
    "        # Upscale im_l to the same size as im_h\n",
    "        im_l = torch.tensor(resize(im_l, im_h.shape, order=1, mode = 'symmetric'))\n",
    "\n",
    "        # Calculate PSNR for bicubic\n",
    "        # im_l = np.rint( np.clip(im_l, 0, 255))\n",
    "        # im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = im_l - im_h\n",
    "        rmse_b = np.sqrt((diff**2).mean())\n",
    "        psnr_b = 20*np.log10(im_h.max()/rmse_b)\n",
    "\n",
    "        #print(f'bicubic evaluation for {idx}: rms={rmse_b}, psnr={psnr_b}')\n",
    "\n",
    "        # Calculate PSNR for SR\n",
    "        # im_h_sr = np.rint( np.clip(im_h_sr, 0, 255))\n",
    "        # im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = output_2 - im_h\n",
    "        rmse_s = np.sqrt((diff**2).mean())\n",
    "        psnr_s = 20*np.log10(im_h.max()/rmse_s)\n",
    "        #print(f'SR evaluation for {idx}: rms={rmse_s}, psnr={psnr_s}')\n",
    "        comp['psnr'].append(psnr_s-psnr_b)\n",
    "        comp['rmse'].append(rmse_s-rmse_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'RMSE: SR - BiC')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check, plot a histogram of how well the SR image performed compared to the Bicubic comparison\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].hist([float(x) for x in comp['psnr'][0:-1:300]], bins=10)\n",
    "axs[0].set_title('PSNR: SR - BiC')\n",
    "axs[1].hist([float(x) for x in comp['rmse'][0:-1:300]], bins=10)\n",
    "axs[1].set_title('RMSE: SR - BiC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Application\n",
    "\n",
    "This is where I test both models on a full MRI image and see how it performs\n",
    "\n",
    "- [ ] Create dataset of slightly altered full MRI `.nii` files\n",
    "- [ ] Load saved trained models\n",
    "- [ ] Compare to simple binomial interpolation (PSNR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testing data for both axes\n",
    "sr = SrGen('../data/CNNIL_nifti/Raw_test/','../data/CNNIL_nifti/Full_test/','../data/CNNIL_nifti/Full_test/')\n",
    "\n",
    "temp = sr.get_template()\n",
    "temp['out_type'] = 'nii'\n",
    "temp['resolution'] = [2,2,2]\n",
    "temp['translation'] = [0, 0, 0]\n",
    "temp['rotation'] = [0, 0, 0]\n",
    "temp['keep_blank'] = False\n",
    "temp['same_size'] = False\n",
    "\n",
    "sr.set_template(temp)\n",
    "\n",
    "# sr_train.run(clear=True, save=True)\n",
    "sr.match_altered(update=True, paths=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full image:\n",
    "# Load matched images\n",
    "im_hr, im_lr = sr.match_altered(update = True, paths=True)\n",
    "\n",
    "# Load trained models:\n",
    "net_1.load_state_dict(torch.load('./MRI_reflect_pad_save_39.p'))\n",
    "net_2.load_state_dict(torch.load('./MRI_reflect_pad_save_39.p'))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in im_hr: #range(len(im_hr)):\n",
    "\n",
    "        # Load in image information\n",
    "        im_h, im_l = sr.load_image_pair(i)\n",
    "\n",
    "        # Take low resolution and upscale using bicubic interpolation\n",
    "        # (which has already been done due to the image generation process)\n",
    "        # Thus im_l is the bicubic interpolation to compare to...\n",
    "\n",
    "        # Use SR model on low resolution image\n",
    "        im_h_sr = net(torch.unsqueeze(torch.unsqueeze(torch.tensor(im_l, dtype=torch.float32),0),0),2)\n",
    "\n",
    "        # Calculate PSNR for bicubic\n",
    "        # im_l = np.rint( np.clip(im_l, 0, 255))\n",
    "        # im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = im_l - im_h\n",
    "        rmse = np.sqrt((diff**2).mean())\n",
    "        psnr = 20*np.log10(np.max(im_h)/rmse)\n",
    "\n",
    "        print(f'bicubic evaluation for {i}: rms={rmse}, psnr={psnr}')\n",
    "\n",
    "        # Calculate PSNR for SR\n",
    "        # im_h_sr = np.rint( np.clip(im_h_sr, 0, 255))\n",
    "        # im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = im_h_sr - im_h\n",
    "        rmse = np.sqrt((diff**2).mean())\n",
    "        psnr = 20*np.log10(np.max(im_h)/rmse)\n",
    "        print(f'SR evaluation for {i}: rms={rmse}, psnr={psnr}')\n",
    "\n",
    "        # if save_pred:\n",
    "        #     img_name = os.path.splitext(os.path.basename(i))[0]\n",
    "        #     Image.fromarray(np.rint(im_h_sr).astype(np.uint8)).save(f\"{save_dir}/{img_name}_SR.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539b544e2c3fdc58492248d082a132f5e0b4fea63e914fb274c32873997cf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
