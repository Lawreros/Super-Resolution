{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a replication/exploration of the paper listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..') # Stupid thing Python makes you do to import from a sibling directory\n",
    "from gen_utils.SrGen import SrGen # Custom class for image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNIL(nn.Module):\n",
    "    def __init__(self, upscale=2, axs = 'hw'):\n",
    "        super().__init__()\n",
    "        self.axs = axs\n",
    "\n",
    "        self.rel = nn.ReLU()\n",
    "        if axs == 'hw':\n",
    "            self.conv1 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv2 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv3 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv4 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv5 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv6 = nn.Conv2d(32,4,3, padding='same', bias=False)\n",
    "            \n",
    "            # Upscale step occurs here\n",
    "\n",
    "            self.conv7 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv8 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv9 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv10 = nn.Conv2d(32,1,3, padding='same', bias = False)\n",
    "        elif axs == 'h' or axs == 'w':\n",
    "            self.conv1 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv2 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv3 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv4 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv5 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv6 = nn.Conv2d(32,2,3, padding='same', bias=False)\n",
    "            \n",
    "            # Upscale step occurs here\n",
    "\n",
    "            self.conv7 = nn.Conv2d(1,32,3, padding='same', bias=False)\n",
    "            self.conv8 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv9 = nn.Conv2d(32,32,3, padding='same', bias=False)\n",
    "            self.conv10 = nn.Conv2d(32,1,3, padding='same', bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.rel(self.conv1(x))\n",
    "        \n",
    "        x_l = self.rel(self.conv2(x))\n",
    "        x_l = self.rel(self.conv3(x_l))\n",
    "        x_l = self.rel(self.conv4(x_l+x))\n",
    "        x_l = self.rel(self.conv5(x_l))\n",
    "        x_l = self.rel(self.conv6(x_l+x))\n",
    "\n",
    "        x_l = self.kern_upscale(x_l, axs = self.axs)\n",
    "\n",
    "        x = self.rel(self.conv7(x_l))\n",
    "        x_h = self.rel(self.conv8(x))\n",
    "        x_h = self.rel(self.conv9(x_h))\n",
    "        x_h = self.rel(self.conv10(x_h+x))\n",
    "\n",
    "        return x_l, x_h #Return both results for the loss function\n",
    "\n",
    "    @staticmethod\n",
    "    def kern_upscale(x, axs='hw'):\n",
    "        # Function to do the unique upscaling pattern they propose in\n",
    "        # the paper\n",
    "        #TODO: have them input a tuple for scale of the dimensions they wish to expand along\n",
    "        s, c, h, w = [int(_) for _ in list(x.shape)]\n",
    "\n",
    "        if axs == 'hw':\n",
    "            c = int(c/2)\n",
    "            x_up = torch.cat(torch.unbind(x,1),2)\n",
    "            x_up = torch.reshape(x_up,(s,1,h*c,w*c))\n",
    "            x_up = torch.transpose(x_up,2,3)\n",
    "            x_up = torch.cat(torch.split(x_up,2,2),3)\n",
    "            x_up = torch.transpose(torch.reshape(x_up,(s,1,h*c,w*c)),2,3)\n",
    "        elif axs == 'h':\n",
    "            x_up = torch.unsqueeze(torch.cat(torch.unbind(x,2),1),1)\n",
    "            #x_up = torch.unsqueeze(x_up,0)\n",
    "        elif axs == 'w':\n",
    "            x_up = torch.cat(torch.unbind(x,3),1)\n",
    "            x_up = torch.unsqueeze(torch.transpose(x_up,1,2),1)\n",
    "            #x_up = torch.unsqueeze(x_up,0)\n",
    "        else:\n",
    "            print('No valid scaling dimension selected, returning False')\n",
    "            x_up = False\n",
    "\n",
    "        return x_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing of kern_upscale function with only 3D tensor\n",
    "\n",
    "a = torch.tensor([[[1,2],[3,4]],\n",
    "                [[5,6],[7,8]],\n",
    "                [[9,10],[11,12]],\n",
    "                [[13,14],[15,16]]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 2])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  5,  2,  6],\n",
       "        [ 9, 13, 10, 14],\n",
       "        [ 3,  7,  4,  8],\n",
       "        [11, 15, 12, 16]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatinate each of the layers next to eachother\n",
    "print(a.shape)\n",
    "b = torch.cat(torch.unbind(a),1)\n",
    "print(b.shape)\n",
    "b = torch.reshape(b,(4,4))\n",
    "print(b.shape)\n",
    "b = torch.transpose(b,0,1)\n",
    "b = torch.cat(torch.split(b,2,0),1)\n",
    "torch.transpose(torch.reshape(b,(4,4)),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a is torch.Size([1, 4, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  5,  2,  6],\n",
       "          [ 9, 13, 10, 14],\n",
       "          [ 3,  7,  4,  8],\n",
       "          [11, 15, 12, 16]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing of above kern_upscale function with 4D tensor (what the model will acually use)\n",
    "a = torch.tensor([[[[1,2],[3,4]],\n",
    "                [[5,6],[7,8]],\n",
    "                [[9,10],[11,12]],\n",
    "                [[13,14],[15,16]]]])\n",
    "print(f'shape of a is {a.shape}')\n",
    "\n",
    "b = torch.cat(torch.unbind(a,1),2)\n",
    "b = torch.reshape(b,(1,1,4,4))\n",
    "b = torch.transpose(b,2,3)\n",
    "b = torch.cat(torch.split(b,2,2),3)\n",
    "b = torch.transpose(torch.reshape(b,(1,1,4,4)),2,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a is torch.Size([2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  5,  2,  6],\n",
       "          [ 3,  7,  4,  8]]],\n",
       "\n",
       "\n",
       "        [[[ 2, 10,  4, 12],\n",
       "          [ 6, 14,  8, 16]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upscale along just one axis\n",
    "a = torch.tensor([[[[1,2],[3,4]],\n",
    "                [[5,6],[7,8]]],[[[2,4],[6,8]],[[10,12],[14,16]]]])\n",
    "print(f'shape of a is {a.shape}')\n",
    "\n",
    "# Version for doubling height\n",
    "# b = torch.cat(torch.unbind(a,2),2)\n",
    "# b = torch.reshape(b,(1,1,2,4))\n",
    "# b = torch.cat(torch.split(b,2,3),2)\n",
    "\n",
    "## b = torch.unsqueeze(torch.cat(torch.unbind(a,2),1),0)\n",
    "\n",
    "# Version for doubling width\n",
    "b = torch.cat(torch.unbind(a,3),1)\n",
    "torch.unsqueeze(torch.transpose(b, 1,2),1)\n",
    "\n",
    "#b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1 = CNNIL(axs = 'hw')\n",
    "net_2 = CNNIL(axs = 'w')\n",
    "\n",
    "# \"... trained the CNN for 40 epochs, starting with a learning rate of 0.001 and decreasing\n",
    "# the learning rate to 0.0001 after the first 20 epochs\"\n",
    "optimizer_1 = optim.Adam(net_1.parameters(), lr=0.001)\n",
    "optimizer_2 = optim.Adam(net_2.parameters(), lr=0.001)\n",
    "\n",
    "# They have a custom loss function that incorporates the final results and the result\n",
    "# right after the upscaling step\n",
    "# https://discuss.pytorch.org/t/custom-loss-functions/29387\n",
    "\n",
    "def intermediate_loss(output_intermediate, output_final, target):\n",
    "    mae_loss = nn.L1Loss() #Built in mean absolute error loss function\n",
    "    loss = mae_loss(output_intermediate, target)+mae_loss(output_final, target)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR and LR file locations updated\n"
     ]
    }
   ],
   "source": [
    "sr_train_1 = SrGen('../data/CNNIL_nifti/Raw/','../data/CNNIL_nifti/HR_patches_ax/','../data/CNNIL_nifti/LR_patches_ax/')\n",
    "\n",
    "temp = sr_train_1.get_template()\n",
    "temp['out_type'] = 'nii'\n",
    "temp['resolution'] = [2,2,1]\n",
    "temp['translation'] = [0, 0, 0]\n",
    "temp['rotation'] = [0, 0, 0]\n",
    "temp['keep_blank'] = False\n",
    "temp['same_size'] = False\n",
    "temp['patch'] = [14,14,1] #[x,y,z] when looking at the brain from the top down\n",
    "temp['step'] = [10,10,2]\n",
    "sr_train_1.set_template(temp)\n",
    "\n",
    "# sr_train.run(clear=True, save=True)\n",
    "sr_train_1.match_altered(update=True, paths=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR and LR file locations updated\n"
     ]
    }
   ],
   "source": [
    "sr_train_2 = SrGen('../data/CNNIL_nifti/Raw/','../data/CNNIL_nifti/HR_patches_2/','../data/CNNIL_nifti/LR_patches_2/')\n",
    "temp = sr_train_2.get_template()\n",
    "temp['out_type'] = 'nii'\n",
    "temp['resolution'] = [1,1,2]\n",
    "temp['translation'] = [0, 0, 0]\n",
    "temp['rotation'] = [0, 0, 0]\n",
    "# temp['scale']= [1,1,1]\n",
    "temp['keep_blank'] = False\n",
    "temp['same_size'] = False\n",
    "temp['patch'] = [1,7,14] #[x,y,z] when looking at the brain from the top down\n",
    "temp['step'] = [2,20,20]\n",
    "sr_train_2.set_template(temp)\n",
    "\n",
    "# sr_train_2.run(clear=True, save=True)\n",
    "sr_train_2.match_altered(update=True, paths=False, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sr_class, axs='hw'):\n",
    "        self.sr_class = sr_class\n",
    "        self.axs = axs\n",
    "\n",
    "        # In case I forget to run match_altered before pulling the class\n",
    "        if not sr_class.HR_files:\n",
    "            sr_class.match_altered(update=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sr_class.HR_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        Y, X = self.sr_class.load_image_pair(index)\n",
    "\n",
    "        # SR along two axes\n",
    "        if self.axs == 'hw':\n",
    "            X = torch.unsqueeze(torch.squeeze(torch.tensor(X, dtype=torch.float32),-1),0)\n",
    "            Y = torch.unsqueeze(torch.squeeze(torch.tensor(Y, dtype=torch.float32),-1),0)\n",
    "\n",
    "        # SR along one axis\n",
    "        elif self.axs == 'h':\n",
    "            X = torch.squeeze(torch.tensor(X, dtype=torch.float32),-1)\n",
    "            Y = torch.squeeze(torch.tensor(Y, dtype=torch.float32),-1)\n",
    "        elif self.axs == 'w':\n",
    "            raise NotImplementedError('Super resolution along width not currently supported')\n",
    "        else:\n",
    "            raise NotImplementedError(f'Super resolution along {self.axs} currently not supported')\n",
    "            \n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 128,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 3}\n",
    "\n",
    "training_set = Dataset(sr_train_2, axs = 'h')\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 0, loss mean: 24.912885665893555, loss: 15.040494918823242-33.60326385498047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:12<03:52, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 1, loss mean: 14.637580267588298, loss: 11.10688591003418-17.60952377319336\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:25<03:54, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 2, loss mean: 10.707223828633627, loss: 9.399085998535156-12.473265647888184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:38<03:39, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 3, loss mean: 8.899015919367473, loss: 7.102175712585449-10.309595108032227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:52<03:30, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 4, loss mean: 7.801561673482259, loss: 5.87510871887207-8.834814071655273\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:05<03:15, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 5, loss mean: 7.312359364827474, loss: 6.32581901550293-8.506750106811523\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:17<03:02, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 6, loss mean: 7.236243979136149, loss: 6.256368637084961-10.29130744934082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:30<02:49, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 7, loss mean: 6.600006357828776, loss: 5.439182281494141-7.585395812988281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:43<02:35, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 8, loss mean: 6.333966286977132, loss: 5.639723300933838-7.310443878173828\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:56<02:22, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 9, loss mean: 6.035986773173014, loss: 5.556248664855957-6.646017074584961\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:09<02:09, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 10, loss mean: 6.049953969319661, loss: 5.268136978149414-7.118051528930664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:22<01:56, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 11, loss mean: 5.830224482218425, loss: 5.293516159057617-6.588449001312256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [02:35<01:42, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 12, loss mean: 5.876652383804322, loss: 5.022849082946777-6.938161849975586\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [02:48<01:30, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 13, loss mean: 5.791742324829102, loss: 5.274097919464111-6.431184768676758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [03:01<01:17, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 14, loss mean: 5.625034379959106, loss: 5.093735218048096-6.374768257141113\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [03:14<01:04, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 15, loss mean: 5.56691214243571, loss: 4.649538993835449-6.418514251708984\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:26<00:51, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 16, loss mean: 5.694764693578084, loss: 5.201357841491699-6.254780292510986\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [03:39<00:38, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 17, loss mean: 5.521954838434855, loss: 4.831023693084717-6.157052993774414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [03:51<00:25, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 18, loss mean: 5.607043838500976, loss: 4.9099507331848145-6.618631362915039\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [04:04<00:12, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch 19, loss mean: 5.419387865066528, loss: 4.683772087097168-6.107345104217529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:16<00:00, 12.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "max_epochs = 20\n",
    "save_rate = 10\n",
    "epoch_adjust = 0\n",
    "save_prefix = \"./CNNIL_save_\"\n",
    "\n",
    "mean_loss = []\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    losses = []\n",
    "\n",
    "    ###### Test running this code where each epoch a new set of random images is made\n",
    "    # sr_train.run(clear=True)\n",
    "\n",
    "    # training_set = Dataset(sr_train)\n",
    "    # training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "    ######\n",
    "\n",
    "\n",
    "    # Training\n",
    "    count = 0\n",
    "    for inp, goal in training_generator:\n",
    "        optimizer_2.zero_grad()\n",
    "\n",
    "        output_1, output_2 = net_2(inp) # the 2 is the number of iterations in the LISTA network\n",
    "        #output = torch.clamp(output, 0, 255)\n",
    "\n",
    "        loss = intermediate_loss(output_1,output_2,goal)\n",
    "        loss.backward()\n",
    "        optimizer_2.step()\n",
    "        #print(f'loss = {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "        #print(f'mini-batch # {count}, mean loss = {sum(losses)/len(losses)}')\n",
    "        count = count+1\n",
    "    \n",
    "\n",
    "    # if (epoch % save_rate == 0) or epoch == (max_epochs-1):\n",
    "    #     torch.save(net.state_dict(), f'{save_prefix}{epoch+epoch_adjust}.p')\n",
    "    print(f'\\n epoch {epoch}, loss mean: {sum(losses)/len(losses)}, loss: {min(losses)}-{max(losses)}\\n')\n",
    "    mean_loss.append(sum(losses)/len(losses))\n",
    "\n",
    "    # Give computer time to cool down\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f155e54d940>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot([x for x in range(len(mean_loss))],mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539b544e2c3fdc58492248d082a132f5e0b4fea63e914fb274c32873997cf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
