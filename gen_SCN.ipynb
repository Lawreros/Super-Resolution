{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCN Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from gen_utils.sr_gen import sr_gen # Custom class for image generation/organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCN(nn.Module):\n",
    "    def __init__(self,sy,sg, train=True):\n",
    "        super().__init__()\n",
    "        C = 5\n",
    "        L = 5\n",
    "\n",
    "        Dx = torch.normal(0,1, size = (25,128))\n",
    "        Dy = torch.normal(0,1, size = (100,128))\n",
    "        I = torch.eye(128)\n",
    "\n",
    "        self.conv = nn.Conv2d(1,100,9, bias = False, stride =1, padding = 6, padding_mode='reflect')\n",
    "        self.mean2 = nn.Conv2d(1,1,13, bias = False, stride = 1, padding = 6, padding_mode='reflect')\n",
    "        self.diffms = nn.Conv2d(1,25,9, bias=False, stride = 1, padding=6, padding_mode='reflect')\n",
    "\n",
    "        self.wd = nn.Conv2d(100,128,1,bias = False, stride = 1)\n",
    "        self.usd1 = nn.Conv2d(128, 128, 1, bias = False, stride=1)\n",
    "        self.ud = nn.Conv2d(128,25,1,bias=False,stride=1)\n",
    "        self.addp = nn.Conv2d(16,1,1, bias = False, stride = 1)\n",
    "\n",
    "        if train: #If you are currently training the model\n",
    "            self.mean2.weight = torch.nn.Parameter(self.create_gaus(13), requires_grad = False)\n",
    "            self.diffms.weight = torch.nn.Parameter(self.create_diffms(9,5),requires_grad=False)\n",
    "            self.wd.weight = torch.nn.Parameter(self.expand_params(C*Dy.T), requires_grad=True)\n",
    "            self.usd1.weight = torch.nn.Parameter(self.expand_params(I - torch.matmul(Dy.T,Dy)), requires_grad=True)\n",
    "            self.ud.weight = torch.nn.Parameter(self.expand_params((1/(C*L))*Dx), requires_grad=True)\n",
    "            self.addp.weight = torch.nn.Parameter(torch.ones(1,16,1,1)*0.06, requires_grad=True)\n",
    "\n",
    "        else:\n",
    "            self.conv.weight = torch.nn.Parameter(torch.ones(100,1,9,9),requires_grad=False)\n",
    "            self.mean2.weight = torch.nn.Parameter(self.create_gaus(13),requires_grad=False)\n",
    "            self.diffms.weight = torch.nn.Parameter(self.create_diffms(9,5),requires_grad=False)\n",
    "            self.wd.weight = torch.nn.Parameter(self.expand_params(C*Dy.T),requires_grad=False)\n",
    "            self.usd1.weight = torch.nn.Parameter(self.expand_params(I - torch.matmul(Dy.T,Dy)),requires_grad=False)\n",
    "            self.ud.weight = torch.nn.Parameter(self.expand_params((1/(C*L))*Dx),requires_grad=False)\n",
    "            self.addp.weight = torch.nn.Parameter(torch.ones(1,16,1,1)*0.06,requires_grad=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, k, sy=9, sg=5):\n",
    "        #print(f'input: {x.min()}-{x.max()}')\n",
    "        x = x+0.1\n",
    "\n",
    "        im_mean = self.mean2(x)\n",
    "        # print(f'im_mean shape {im_mean.shape}')\n",
    "        diffms = self.diffms(x)\n",
    "        # print(f'diffms shape: {diffms.shape}')\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "        # y = torch.zeros(n, 100, h-8, w-8)\n",
    "        x = self.conv(x)\n",
    "        # print(f'post conv shape {x.shape}')\n",
    "        #print(f'conv max {x.max()}')\n",
    "        #x=x+1\n",
    "\n",
    "        x = x/torch.linalg.vector_norm(x, ord=2, dim=1, keepdim=True)\n",
    "        # print(f'post vector norm shape: {x.shape}')\n",
    "        #print(f'postnorm max {x.max()}')\n",
    "\n",
    "        x = self.wd(x)\n",
    "        #print(f'conv wd {x.max()}')\n",
    "        z = self.ShLU(x,1)\n",
    "        #print(f'conv SHLU {x.max()}')\n",
    "\n",
    "        # Go through LISTA\n",
    "        for i in range(k):\n",
    "            z = self.ShLU(self.usd1(z)+x,1)\n",
    "\n",
    "        x = self.ud(z)\n",
    "        #print(f'ud max {x.max()}')\n",
    "        # print(f'post ud shape {x.shape}')\n",
    "        x = (x/torch.linalg.vector_norm(x, ord=2, dim=1, keepdim=True))*torch.linalg.vector_norm(diffms, ord=2, dim=1, keepdim=True)*1.1\n",
    "        # print(f'prereassembled x shape {x.shape}')\n",
    "        x = self.reassemble2(x,im_mean,4)\n",
    "        # print(f'reassembled x shape {x.shape}')\n",
    "        x = self.addp(x)\n",
    "        #print(f'x.reassemble.max = {x.max()}')\n",
    "        x = x+im_mean\n",
    "\n",
    "        #print(f'output: {x.min()}-{x.max()}')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reassemble2(self, x, im_mean, patch_size):\n",
    "        img = im_mean\n",
    "        s, c, h, w = img.shape\n",
    "        \n",
    "        # img_stack=torch.zeros(s,25,h,w)\n",
    "        img_stack=torch.zeros(s,16,h,w)\n",
    "        \n",
    "        #go through every sample and reassemble the image\n",
    "        for q in range(x.shape[0]):\n",
    "            filt = 0\n",
    "            for ii in range(patch_size-1, -1, -1):\n",
    "                for jj in range(patch_size-1, -1, -1):\n",
    "                    img_stack[q,filt,:,:] = x[q,filt,jj:(jj+h), ii:(ii+w)]\n",
    "                    filt+=1\n",
    "        \n",
    "        return img_stack\n",
    "    \n",
    "    def create_diffms(self, kern_size, sy=5):\n",
    "        diffms = torch.zeros(sy**2,1,kern_size,kern_size)\n",
    "        \n",
    "        neg = -1*(1/(sy**2))\n",
    "        pos = 1+neg\n",
    "        \n",
    "        border = int((kern_size-sy)/2)\n",
    "        base = torch.zeros(sy,sy)+neg\n",
    "        cnt=0\n",
    "        \n",
    "        for i in range(sy**2):\n",
    "            base = torch.zeros(sy**2)+neg\n",
    "            base[cnt]=pos\n",
    "            diffms[i,0,border:(kern_size-border),border:(kern_size-border)] = base.reshape([sy,sy])\n",
    "            cnt+=1\n",
    "        return diffms\n",
    "    \n",
    "    \n",
    "    def create_gaus(self, kern_size, sy=9,std=2.15):\n",
    "        n = torch.arange(0,sy)-(sy-1.0)/2.0\n",
    "        sig2 = 2 * std * std\n",
    "        gkern1d = torch.exp(-n ** 2 / sig2)\n",
    "        gkern1d = gkern1d/torch.sum(gkern1d)\n",
    "        #print(gkern1d.shape)\n",
    "        gkern2d = torch.outer(gkern1d, gkern1d)\n",
    "    \n",
    "\n",
    "        # Wrap in zeros, if kern_size > sy\n",
    "        gaussian_filter = torch.zeros(1,1,kern_size,kern_size)\n",
    "        border = int((kern_size-sy)/2)\n",
    "        gaussian_filter[0,0,border:(kern_size-border),border:(kern_size-border)] = gkern2d#(sy,std=std)\n",
    "        #print(gaussian_filter.shape)\n",
    "        return gaussian_filter\n",
    "        \n",
    "    \n",
    "    def fixed_positions(self, tens, mult, sg):\n",
    "        f, _ , h, w = tens.shape\n",
    "        new_filt = torch.zeros(f*mult, 1, sg,sg)\n",
    "        cnt = 0\n",
    "        filt = 0\n",
    "        \n",
    "        for filt in range(f):\n",
    "            for j in range((sg-w)+1):\n",
    "                for i in range((sg-h)+1):\n",
    "                    new_filt[cnt,0,i:i+h,j:j+w] = tens[filt]\n",
    "                    cnt+=1\n",
    "        return new_filt\n",
    "    \n",
    "    def expand_params(self,tens):\n",
    "        return torch.unsqueeze(torch.unsqueeze(tens,2),3)\n",
    "    \n",
    "    def ShLU(self,a, th):\n",
    "        return torch.sign(a)*torch.maximum(abs(a)-th, torch.tensor(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SCN(9,5,train=True)\n",
    "\n",
    "#net.load_state_dict(torch.load('./MRI_save_29.p'))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        {\"params\": net.addp.parameters()},#, \"lr\": 0.0002, \"momentum\": 0.00005},\n",
    "        {\"params\": net.conv.parameters()},#, \"lr\": 0.0003, \"momentum\": 0.0001},\n",
    "        {\"params\": net.wd.parameters()},\n",
    "        {\"params\": net.usd1.parameters()},\n",
    "        {\"params\": net.ud.parameters()},\n",
    "    ],\n",
    "    #lr=0.0001, momentum=0.0001\n",
    "    lr=0.00007, momentum = 0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_train = sr_gen('./data/train/GT_corr/','./data/train/HR_corr_patches/','./data/train/LR_corr_patches/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n"
     ]
    }
   ],
   "source": [
    "temp = sr_train.get_template()\n",
    "temp[\"patch\"]=44\n",
    "temp[\"step\"]=20\n",
    "temp[\"translation_x\"]=10\n",
    "temp[\"translation_y\"]=10\n",
    "temp[\"rotation\"] = 180\n",
    "temp[\"scale\"] = 1\n",
    "sr_train.save_template(temp)\n",
    "\n",
    "sr_train.run(clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: There's not reasing I can't combine the Dataset class and my custom class into one thing\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sr_class):\n",
    "        self.sr_class = sr_class\n",
    "\n",
    "        # In case I forget to run match_altered before pulling the class\n",
    "        if not sr_class.HR_files:\n",
    "            sr_class.match_altered(update=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sr_class.HR_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        Y, X = self.sr_class.load_image_pair(index)\n",
    "        X = torch.unsqueeze(torch.tensor(X, dtype=torch.float32),0)\n",
    "        Y = torch.unsqueeze(torch.tensor(Y, dtype=torch.float32),0)\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 3}\n",
    "\n",
    "training_set = Dataset(sr_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 0, loss mean: 98.32311543551359, loss: 54.4627799987793-167.3396759033203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [03:28<2:15:19, 208.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 1, loss mean: 51.35435017672452, loss: 42.5849609375-68.5630874633789\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [06:57<2:12:15, 208.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 2, loss mean: 41.78303077004173, loss: 34.81265640258789-50.28232192993164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [10:25<2:08:32, 208.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 3, loss mean: 36.33278135819869, loss: 33.42055892944336-41.9842643737793\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [13:57<2:05:49, 209.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 4, loss mean: 33.58856357227672, loss: 29.76911735534668-36.285865783691406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [17:25<2:01:59, 209.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 5, loss mean: 31.985302665016867, loss: 27.908756256103516-37.955238342285156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [20:46<1:57:01, 206.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 6, loss mean: 30.406118653037332, loss: 26.195032119750977-35.40159225463867\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [24:07<1:52:31, 204.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 7, loss mean: 30.59968558224765, loss: 26.082624435424805-37.712486267089844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [27:27<1:48:27, 203.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 8, loss mean: 30.362801638516512, loss: 26.34787940979004-36.843509674072266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [30:47<1:44:26, 202.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 9, loss mean: 29.200725728815254, loss: 24.18450927734375-33.6534423828125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [34:08<1:40:54, 201.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 10, loss mean: 28.70171512256969, loss: 23.59297752380371-32.42100524902344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [37:30<1:37:30, 201.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 11, loss mean: 29.119379997253418, loss: 25.405202865600586-33.913997650146484\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [40:49<1:33:48, 201.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 12, loss mean: 29.129417072642934, loss: 24.721755981445312-31.894960403442383\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [44:09<1:30:18, 200.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 13, loss mean: 27.569858290932395, loss: 23.46613311767578-32.30982208251953\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [47:28<1:26:46, 200.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 14, loss mean: 29.800518642772328, loss: 26.081289291381836-33.39929962158203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [50:47<1:23:18, 199.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 15, loss mean: 29.03403230146928, loss: 25.16359519958496-33.611568450927734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [54:06<1:19:51, 199.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 16, loss mean: 27.83819432692094, loss: 21.973224639892578-34.02215576171875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [57:24<1:16:18, 199.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 17, loss mean: 26.745600353587758, loss: 22.799522399902344-30.931591033935547\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [1:00:43<1:12:59, 199.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 18, loss mean: 27.71150849082253, loss: 24.253095626831055-32.837615966796875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [1:04:03<1:09:43, 199.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 19, loss mean: 27.18992978876287, loss: 23.9979190826416-31.281557083129883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [1:07:21<1:06:16, 198.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 20, loss mean: 26.32117020000111, loss: 23.067411422729492-30.41972541809082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [1:10:39<1:02:55, 198.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 21, loss mean: 26.325577995993875, loss: 24.279020309448242-28.89850425720215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [1:13:59<59:41, 198.95s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 22, loss mean: 27.708439480174672, loss: 23.66593360900879-31.992412567138672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [1:17:19<56:28, 199.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 23, loss mean: 28.123160535638984, loss: 23.07906150817871-31.367679595947266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [1:20:38<53:08, 199.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 24, loss mean: 27.850334600968793, loss: 24.9620304107666-30.15374183654785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [1:23:58<49:51, 199.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 25, loss mean: 27.12289506738836, loss: 23.964746475219727-30.789180755615234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [1:27:16<46:28, 199.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 26, loss mean: 25.82103018327193, loss: 21.289731979370117-29.91715431213379\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [1:30:36<43:11, 199.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 27, loss mean: 25.821901321411133, loss: 22.979354858398438-31.2678165435791\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [1:33:57<39:56, 199.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 28, loss mean: 25.819851875305176, loss: 22.628971099853516-28.7922306060791\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [1:37:18<36:41, 200.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 29, loss mean: 25.583398298783735, loss: 22.019546508789062-28.75263023376465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [1:40:39<33:24, 200.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 30, loss mean: 25.906915577975187, loss: 23.25433349609375-28.14934539794922\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [1:43:58<30:01, 200.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 31, loss mean: 27.709976196289062, loss: 23.960636138916016-32.19546127319336\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [1:47:18<26:39, 199.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 32, loss mean: 25.40254020690918, loss: 20.6865177154541-28.738365173339844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [1:50:37<23:17, 199.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 33, loss mean: 25.636878880587492, loss: 22.34256362915039-29.13888931274414\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [1:53:57<19:58, 199.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 34, loss mean: 25.631733547557484, loss: 21.948619842529297-28.845075607299805\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [1:57:17<16:40, 200.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 35, loss mean: 26.581252444874156, loss: 23.607032775878906-31.140268325805664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [2:00:39<13:22, 200.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 36, loss mean: 25.556747089732777, loss: 22.92979621887207-28.44339942932129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [2:03:59<10:01, 200.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 37, loss mean: 25.338418440385297, loss: 22.199262619018555-29.149282455444336\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [2:07:21<06:41, 200.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 38, loss mean: 25.892704963684082, loss: 23.212108612060547-29.126216888427734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [2:10:42<03:20, 200.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "\n",
      "\n",
      " epoch 39, loss mean: 26.585525946183637, loss: 21.103351593017578-30.358949661254883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [2:14:03<00:00, 201.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "# Loop over epochs\n",
    "\n",
    "max_epochs = 40\n",
    "save_rate = 5 #save a version of the model every 5 epochs\n",
    "epoch_adjust = 0 #how much to add to the saved files in order to not overwrite\n",
    "save_prefix = \"./MRI_reflect_pad_save_\"\n",
    "\n",
    "mean_loss = []\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    losses = []\n",
    "\n",
    "    ###### Test running this code where each epoch a new set of random images is made\n",
    "    sr_train.run(clear=True)\n",
    "\n",
    "    training_set = Dataset(sr_train)\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "    ######\n",
    "\n",
    "\n",
    "    # Training\n",
    "    count = 0\n",
    "    for inp, goal in training_generator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(inp,2) # the 2 is the number of iterations in the LISTA network\n",
    "        output = torch.clamp(output, 0, 255)\n",
    "\n",
    "        loss = criterion(output,goal)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f'loss = {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "        #print(f'mini-batch # {count}, mean loss = {sum(losses)/len(losses)}')\n",
    "        count = count+1\n",
    "\n",
    "    if (epoch % save_rate == 0) or epoch == (max_epochs-1):\n",
    "        torch.save(net.state_dict(), f'{save_prefix}{epoch+epoch_adjust}.p')\n",
    "    print(f'\\n\\n epoch {epoch}, loss mean: {sum(losses)/len(losses)}, loss: {min(losses)}-{max(losses)}\\n')\n",
    "    mean_loss.append(sum(losses)/len(losses))\n",
    "\n",
    "    # Give computer time to cool down\n",
    "    time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.94681098244407]\n"
     ]
    }
   ],
   "source": [
    "print(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testing data\n",
    "sr_test = sr_gen('./data/test/GT_corr/','./data/test/HR_corr/','./data/test/LR_corr/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directories\n",
      "Saving image: 155_rot91\n",
      "Saving image: 140_rot67\n",
      "Saving image: 145_rot-144\n",
      "Saving image: 160_rot-1\n",
      "Saving image: 150_rot150\n"
     ]
    }
   ],
   "source": [
    "temp = sr_test.get_template()\n",
    "temp[\"patch\"]=False #44\n",
    "# temp[\"step\"]=20\n",
    "# temp[\"rotation\"] = 180\n",
    "temp[\"scale\"] = 1\n",
    "temp[\"rotation\"]=180\n",
    "sr_test.save_template(temp)\n",
    "\n",
    "sr_test.run(clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL: Load previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SCN(9,5,train=False) #Switch to True if you want to keep training\n",
    "net.load_state_dict(torch.load('./MRI_reflect_pad_save_39.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR and LR file locations updated\n",
      "bicubic evaluation for ./data/test/HR_corr/155_rot91.png: rms=4.91015625, psnr=34.30889736119694\n",
      "SR evaluation for ./data/test/HR_corr/155_rot91.png: rms=4.818031311035156, psnr=34.473411560058594\n",
      "bicubic evaluation for ./data/test/HR_corr/145_rot-144.png: rms=4.546875, psnr=34.976543308638696\n",
      "SR evaluation for ./data/test/HR_corr/145_rot-144.png: rms=4.458677768707275, psnr=35.14668273925781\n",
      "bicubic evaluation for ./data/test/HR_corr/140_rot67.png: rms=3.361328125, psnr=37.60058542164452\n",
      "SR evaluation for ./data/test/HR_corr/140_rot67.png: rms=3.3172366619110107, psnr=37.715274810791016\n",
      "bicubic evaluation for ./data/test/HR_corr/150_rot150.png: rms=5.20703125, psnr=33.79899992663891\n",
      "SR evaluation for ./data/test/HR_corr/150_rot150.png: rms=5.131485939025879, psnr=33.925941467285156\n",
      "bicubic evaluation for ./data/test/HR_corr/160_rot-1.png: rms=4.40234375, psnr=35.257124593993964\n",
      "SR evaluation for ./data/test/HR_corr/160_rot-1.png: rms=4.329180717468262, psnr=35.402687072753906\n"
     ]
    }
   ],
   "source": [
    "# Load matched images\n",
    "im_hr, im_lr = sr_test.match_altered(update = True, paths=True)\n",
    "\n",
    "save_pred = False # Whether to save the images created by the network during testing\n",
    "save_dir = \"./\"\n",
    "if save_pred:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in im_hr: #range(len(im_hr)):\n",
    "\n",
    "        # Load in image information\n",
    "        im_h, im_l = sr_test.load_image_pair(i)\n",
    "\n",
    "        # Take low resolution and upscale using bicubic interpolation\n",
    "        # (which has already been done due to the image generation process)\n",
    "        # Thus im_l is the bicubic interpolation to compare to...\n",
    "\n",
    "        # Use SR model on low resolution image\n",
    "        im_h_sr = net(torch.unsqueeze(torch.unsqueeze(torch.tensor(im_l, dtype=torch.float32),0),0),2)\n",
    "\n",
    "        # Calculate PSNR for bicubic\n",
    "        im_l = np.rint( np.clip(im_l, 0, 255))\n",
    "        im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = im_l - im_h\n",
    "        rmse = np.sqrt((diff**2).mean())\n",
    "        psnr = 20*np.log10(255.0/rmse)\n",
    "\n",
    "        print(f'bicubic evaluation for {i}: rms={rmse}, psnr={psnr}')\n",
    "\n",
    "        # Calculate PSNR for SR\n",
    "        im_h_sr = np.rint( np.clip(im_h_sr, 0, 255))\n",
    "        im_h = np.rint( np.clip(im_h, 0, 255))\n",
    "        diff = im_h_sr - im_h\n",
    "        rmse = np.sqrt((diff**2).mean())\n",
    "        psnr = 20*np.log10(255.0/rmse)\n",
    "        print(f'SR evaluation for {i}: rms={rmse}, psnr={psnr}')\n",
    "\n",
    "        if save_pred:\n",
    "            img_name = os.path.splitext(os.path.basename(i))[0]\n",
    "            Image.fromarray(np.rint(im_h_sr).astype(np.uint8)).save(f\"{save_dir}/{img_name}_SR.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('3.8.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "539b544e2c3fdc58492248d082a132f5e0b4fea63e914fb274c32873997cf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
